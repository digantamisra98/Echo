
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Welcome to Echo AI documentation! &#8212; EchoAI 2019 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="welcome-to-echo-ai-documentation">
<h1>Welcome to Echo AI documentation!<a class="headerlink" href="#welcome-to-echo-ai-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#about" id="id140">About</a></li>
<li><a class="reference internal" href="#installation" id="id141">Installation</a></li>
<li><a class="reference internal" href="#examples" id="id142">Examples</a></li>
<li><a class="reference internal" href="#pytorch-extensions" id="id143">PyTorch Extensions</a></li>
<li><a class="reference internal" href="#keras-extensions" id="id144">Keras Extensions</a></li>
<li><a class="reference internal" href="#tensorflow-keras-extensions" id="id145">Tensorflow Keras Extensions</a></li>
<li><a class="reference internal" href="#indices-and-tables" id="id146">Indices and tables</a></li>
</ul>
</div>
<div class="section" id="about">
<h2><a class="toc-backref" href="#id140">About</a><a class="headerlink" href="#about" title="Permalink to this headline">¶</a></h2>
<p><strong>EchoAI Package</strong> is created to provide an implementation of the most promising mathematical algorithms, which are missing in the most popular deep learning libraries, such as <a class="reference external" href="https://pytorch.org/">PyTorch</a>, <a class="reference external" href="https://keras.io/">Keras</a> and
<a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a>.</p>
<div class="section" id="implemented-activation-functions">
<h3>Implemented Activation Functions<a class="headerlink" href="#implemented-activation-functions" title="Permalink to this headline">¶</a></h3>
<p>The list of activation functions implemented in Echo:</p>
<table border="1" class="docutils">
<colgroup>
<col width="3%" />
<col width="13%" />
<col width="26%" />
<col width="39%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">#</th>
<th class="head">Activation</th>
<th class="head">PyTorch</th>
<th class="head">Keras</th>
<th class="head">TensorFlow Keras</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>1</td>
<td>Weighted Tanh</td>
<td><a class="reference internal" href="#torch-weightedtanh"><span class="std std-ref">Torch.WeightedTanh</span></a></td>
<td><a class="reference internal" href="#keras-weighted-tanh"><span class="std std-ref">Keras.WeightedTanh</span></a></td>
<td><a class="reference internal" href="#tf-keras-weighted-tanh"><span class="std std-ref">Tensorflow_Keras.WeightedTanh</span></a></td>
</tr>
<tr class="row-odd"><td>2</td>
<td>Aria2</td>
<td><a class="reference internal" href="#torch-aria2"><span class="std std-ref">Torch.aria2</span></a></td>
<td><a class="reference internal" href="#keras-aria2"><span class="std std-ref">Keras.Aria2</span></a></td>
<td><a class="reference internal" href="#tf-keras-aria2"><span class="std std-ref">Tensorflow_Keras.Aria2</span></a></td>
</tr>
<tr class="row-even"><td>3</td>
<td>SiLU</td>
<td><a class="reference internal" href="#torch-silu"><span class="std std-ref">Torch.Silu</span></a></td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td>4</td>
<td>E-Swish</td>
<td><a class="reference internal" href="#torch-eswish"><span class="std std-ref">Torch.Eswish</span></a></td>
<td><a class="reference internal" href="#keras-eswish"><span class="std std-ref">Keras.Eswish</span></a></td>
<td><a class="reference internal" href="#tf-keras-eswish"><span class="std std-ref">Tensorflow_Keras.ESwish</span></a></td>
</tr>
<tr class="row-even"><td>5</td>
<td>Swish</td>
<td><a class="reference internal" href="#torch-swish"><span class="std std-ref">Torch.Swish</span></a></td>
<td><a class="reference internal" href="#keras-swish"><span class="std std-ref">Keras.Swish</span></a></td>
<td><a class="reference internal" href="#tf-keras-swish"><span class="std std-ref">Tensorflow_Keras.Swish</span></a></td>
</tr>
<tr class="row-odd"><td>6</td>
<td>ELiSH</td>
<td><a class="reference internal" href="#torch-elish"><span class="std std-ref">Torch.Elish</span></a></td>
<td><a class="reference internal" href="#keras-elish"><span class="std std-ref">Keras.Elish</span></a></td>
<td><a class="reference internal" href="#tf-keras-elish"><span class="std std-ref">Tensorflow_Keras.ELiSH</span></a></td>
</tr>
<tr class="row-even"><td>7</td>
<td>Hard ELiSH</td>
<td><a class="reference internal" href="#torch-hard-elish"><span class="std std-ref">Torch.HardElish</span></a></td>
<td><a class="reference internal" href="#keras-hard-elish"><span class="std std-ref">Keras.HardElish</span></a></td>
<td><a class="reference internal" href="#tf-keras-hard-elish"><span class="std std-ref">Tensorflow_Keras.HardELiSH</span></a></td>
</tr>
<tr class="row-odd"><td>8</td>
<td>Mila</td>
<td><a class="reference internal" href="#torch-mila"><span class="std std-ref">Torch.Mila</span></a></td>
<td><a class="reference internal" href="#keras-mila"><span class="std std-ref">Keras.Mila</span></a></td>
<td><a class="reference internal" href="#tf-keras-mila"><span class="std std-ref">Tensorflow_Keras.Mila</span></a></td>
</tr>
<tr class="row-even"><td>9</td>
<td>SineReLU</td>
<td><a class="reference internal" href="#torch-sine-relu"><span class="std std-ref">Torch.SineReLU</span></a></td>
<td><a class="reference internal" href="#keras-sinerelu"><span class="std std-ref">Keras.SineReLU</span></a></td>
<td><a class="reference internal" href="#tf-keras-sine-relu"><span class="std std-ref">Tensorflow_Keras.SineReLU</span></a></td>
</tr>
<tr class="row-odd"><td>10</td>
<td>Flatten T-Swish</td>
<td><a class="reference internal" href="#torch-fts"><span class="std std-ref">Torch.FTS</span></a></td>
<td><a class="reference internal" href="#keras-fts"><span class="std std-ref">Keras.FTS</span></a></td>
<td><a class="reference internal" href="#tf-keras-fts"><span class="std std-ref">Tensorflow_Keras.FlattenTSwish</span></a></td>
</tr>
<tr class="row-even"><td>11</td>
<td>SQNL</td>
<td><a class="reference internal" href="#torch-sqnl"><span class="std std-ref">Torch.SQNL</span></a></td>
<td><a class="reference internal" href="#keras-sqnl"><span class="std std-ref">Keras.SQNL</span></a></td>
<td><a class="reference internal" href="#tf-keras-sqnl"><span class="std std-ref">Tensorflow_Keras.SQNL</span></a></td>
</tr>
<tr class="row-odd"><td>12</td>
<td>Mish</td>
<td><a class="reference internal" href="#torch-mish"><span class="std std-ref">Torch.Mish</span></a></td>
<td><a class="reference internal" href="#keras-mish"><span class="std std-ref">Keras.Mish</span></a></td>
<td><a class="reference internal" href="#tf-keras-mish"><span class="std std-ref">Tensorflow_Keras.Mish</span></a></td>
</tr>
<tr class="row-even"><td>13</td>
<td>Beta Mish</td>
<td><a class="reference internal" href="#torch-beta-mish"><span class="std std-ref">Torch.BetaMish</span></a></td>
<td><a class="reference internal" href="#keras-beta-mish"><span class="std std-ref">Keras.BetaMish</span></a></td>
<td><a class="reference internal" href="#tf-keras-beta-mish"><span class="std std-ref">Tensorflow_Keras.BetaMish</span></a></td>
</tr>
<tr class="row-odd"><td>14</td>
<td>ISRU</td>
<td><a class="reference internal" href="#torch-isru"><span class="std std-ref">Torch.ISRU</span></a></td>
<td><a class="reference internal" href="#keras-isru"><span class="std std-ref">Keras.ISRU</span></a></td>
<td><a class="reference internal" href="#tf-keras-isru"><span class="std std-ref">Tensorflow_Keras.ISRU</span></a></td>
</tr>
<tr class="row-even"><td>15</td>
<td>ISRLU</td>
<td><a class="reference internal" href="#torch-isrlu"><span class="std std-ref">Torch.ISRLU</span></a></td>
<td><a class="reference internal" href="#keras-isrlu"><span class="std std-ref">Keras.ISRLU</span></a></td>
<td><a class="reference internal" href="#tf-keras-isrlu"><span class="std std-ref">Tensorflow_Keras.ISRLU</span></a></td>
</tr>
<tr class="row-odd"><td>16</td>
<td>Bent’s Identity</td>
<td><a class="reference internal" href="#torch-bent-id"><span class="std std-ref">Torch.BentID</span></a></td>
<td><a class="reference internal" href="#keras-bent-id"><span class="std std-ref">Keras.BentID</span></a></td>
<td><a class="reference internal" href="#tf-keras-bent-id"><span class="std std-ref">Tensorflow_Keras.BentIdentity</span></a></td>
</tr>
<tr class="row-even"><td>17</td>
<td>Soft Clipping</td>
<td><a class="reference internal" href="#torch-soft-clipping"><span class="std std-ref">Torch.SoftClipping</span></a></td>
<td><a class="reference internal" href="#keras-soft-clipping"><span class="std std-ref">Keras.SoftClipping</span></a></td>
<td><a class="reference internal" href="#tf-keras-soft-clipping"><span class="std std-ref">Tensorflow_Keras.SoftClipping</span></a></td>
</tr>
<tr class="row-odd"><td>18</td>
<td>SReLU</td>
<td><a class="reference internal" href="#torch-srelu"><span class="std std-ref">Torch.SReLU</span></a></td>
<td><a class="reference internal" href="#keras-srelu"><span class="std std-ref">Keras.SReLU</span></a></td>
<td><a class="reference internal" href="#tf-keras-srelu"><span class="std std-ref">Tensorflow_Keras.SReLU</span></a></td>
</tr>
<tr class="row-even"><td>19</td>
<td>BReLU</td>
<td><a class="reference internal" href="#torch-brelu"><span class="std std-ref">Torch.BReLU</span></a></td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#tf-keras-brelu"><span class="std std-ref">Tensorflow_Keras.BReLU</span></a></td>
</tr>
<tr class="row-odd"><td>20</td>
<td>APL</td>
<td><a class="reference internal" href="#torch-apl"><span class="std std-ref">Torch.APL</span></a></td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#tf-keras-apl"><span class="std std-ref">Tensorflow_Keras.APL</span></a></td>
</tr>
<tr class="row-even"><td>21</td>
<td>Soft Exponential</td>
<td><a class="reference internal" href="#torch-soft-exponential"><span class="std std-ref">Torch.SoftExponential</span></a></td>
<td><a class="reference internal" href="#keras-soft-exponential"><span class="std std-ref">Keras.SoftExponential</span></a></td>
<td><a class="reference internal" href="#tf-keras-soft-exponential"><span class="std std-ref">Tensorflow_Keras.SoftExponential</span></a></td>
</tr>
<tr class="row-odd"><td>22</td>
<td>Maxout</td>
<td><a class="reference internal" href="#torch-maxout"><span class="std std-ref">Torch.Maxout</span></a></td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#tf-keras-maxout"><span class="std std-ref">Tensorflow_Keras.MaxOut</span></a></td>
</tr>
<tr class="row-even"><td>23</td>
<td>CELU</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#keras-celu"><span class="std std-ref">Keras.Celu</span></a></td>
<td><a class="reference internal" href="#tf-keras-celu"><span class="std std-ref">Tensorflow_Keras.CELU</span></a></td>
</tr>
<tr class="row-odd"><td>23</td>
<td>ReLU6</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#keras-relu6"><span class="std std-ref">Keras.ReLU6</span></a></td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
</tr>
<tr class="row-even"><td>24</td>
<td>Hard Tanh</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#keras-hard-tanh"><span class="std std-ref">Keras.HardTanh</span></a></td>
<td><a class="reference internal" href="#tf-keras-hard-tanh"><span class="std std-ref">Tensorflow_Keras.HardTanh</span></a></td>
</tr>
<tr class="row-odd"><td>25</td>
<td>Log Sigmoid</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#keras-log-sigmoid"><span class="std std-ref">Keras.LogSigmoid</span></a></td>
<td><a class="reference internal" href="#tf-keras-log-sigmoid"><span class="std std-ref">Tensorflow_Keras.LogSigmoid</span></a></td>
</tr>
<tr class="row-even"><td>26</td>
<td>Tanh Shrink</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#keras-tanh-shrink"><span class="std std-ref">Keras.TanhShrink</span></a></td>
<td><a class="reference internal" href="#tf-keras-tanh-shrink"><span class="std std-ref">Tensorflow_Keras.TanhShrink</span></a></td>
</tr>
<tr class="row-odd"><td>27</td>
<td>Hard Shrink</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#keras-hard-shrink"><span class="std std-ref">Keras.HardShrink</span></a></td>
<td><a class="reference internal" href="#tf-keras-hard-shrink"><span class="std std-ref">Tensorflow_Keras.HardShrink</span></a></td>
</tr>
<tr class="row-even"><td>28</td>
<td>Soft Shrink</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#keras-soft-shrink"><span class="std std-ref">Keras.SoftShrink</span></a></td>
<td><a class="reference internal" href="#tf-keras-soft-shrink"><span class="std std-ref">Tensorflow_Keras.SoftShrink</span></a></td>
</tr>
<tr class="row-odd"><td>29</td>
<td>Softmin</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#keras-softmin"><span class="std std-ref">Keras.SoftMin</span></a></td>
<td><a class="reference internal" href="#tf-keras-softmin"><span class="std std-ref">Tensorflow_Keras.SoftMin</span></a></td>
</tr>
<tr class="row-even"><td>30</td>
<td>LogSoftmax</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
<td><a class="reference internal" href="#keras-log-softmax"><span class="std std-ref">Keras.LogSoftmax</span></a></td>
<td><a class="reference internal" href="#tf-keras-log-softmax"><span class="std std-ref">Tensorflow_Keras.LogSoftMax</span></a></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="installation">
<h2><a class="toc-backref" href="#id141">Installation</a><a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>To install EchoAI package from source follow the instructions below:</p>
<ol class="arabic simple">
<li>Clone or download <a class="reference external" href="https://github.com/digantamisra98/Echo">GitHub repository</a>.</li>
<li>Navigate to <strong>echoAI</strong> folder:</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; $ cd Echo
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li>Install the package with pip:</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; $ pip install .
</pre></div>
</div>
<p>To install EchoAI package from PyPI follow the instructions below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; $ pip install echoAI
</pre></div>
</div>
</div>
<div class="section" id="examples">
<h2><a class="toc-backref" href="#id142">Examples</a><a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<div class="section" id="torch-activation-functions">
<h3>Torch Activation Functions<a class="headerlink" href="#torch-activation-functions" title="Permalink to this headline">¶</a></h3>
<p>The following code block contains an example of usage of a PyTorch activation function
from Echo package:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># import activations from EchoAI</span>
<span class="hll"><span class="kn">from</span> <span class="nn">echoAI.Activation.Torch.weightedTanh</span> <span class="kn">import</span> <span class="n">WeightedTanh</span>
</span><span class="hll"><span class="kn">import</span> <span class="nn">echoAI.Activation.Torch.functional</span> <span class="kn">as</span> <span class="nn">Func</span>
</span>
<span class="c1"># use activations in layers of model defined in class</span>
<span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># initialize layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># make sure the input tensor is flattened</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># apply activation function from Echo</span>
<span class="hll">        <span class="n">x</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">weighted_tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Initialize the model using defined Classifier class</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>

    <span class="c1"># Create model with Sequential</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
                         <span class="p">(</span><span class="s1">&#39;fc1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span>
                         <span class="c1"># use activation function from Echo</span>
<span class="hll">                         <span class="p">(</span><span class="s1">&#39;wtahn1&#39;</span><span class="p">,</span>  <span class="n">WeightedTanh</span><span class="p">(</span><span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)),</span>
</span>                         <span class="p">(</span><span class="s1">&#39;fc2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;bn2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">128</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                         <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;fc3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;bn3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">64</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;relu3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                         <span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;logsoftmax&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))]))</span>
</pre></div>
</div>
</div>
<div class="section" id="keras-activation-functions">
<h3>Keras Activation Functions<a class="headerlink" href="#keras-activation-functions" title="Permalink to this headline">¶</a></h3>
<p>The following code block contains an example of usage of a Keras activation function
from Echo package:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the activation function from Echo package</span>
<span class="hll"><span class="kn">from</span> <span class="nn">echoAI.Activation.Keras.custom_activations</span> <span class="kn">import</span> <span class="n">WeightedTanh</span>
</span>
<span class="c1"># Define the CNN model</span>
<span class="k">def</span> <span class="nf">CNNModel</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implementation of the simple CNN.</span>

<span class="sd">    INPUT:</span>
<span class="sd">        input_shape -- shape of the images of the dataset</span>

<span class="sd">    OUTPUT::</span>
<span class="sd">        model -- a Model() instance in Keras</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Define the input placeholder as a tensor with shape input_shape.</span>
    <span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="c1"># Zero-Padding: pads the border of X_input with zeroes</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">ZeroPadding2D</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">X_input</span><span class="p">)</span>

    <span class="c1"># CONV -&gt; BN -&gt; Activation Block applied to X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;conv0&#39;</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;bn0&#39;</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Use custom activation function from Echo package</span>
<span class="hll">    <span class="n">X</span> <span class="o">=</span> <span class="n">WeightedTanh</span><span class="p">()(</span><span class="n">X</span><span class="p">)</span>
</span>
    <span class="c1"># MAXPOOL</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;max_pool&#39;</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># FLATTEN X (means convert it to a vector) + FULLYCONNECTED</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc&#39;</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Create model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">X_input</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;CNNModel&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># Create the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CNNModel</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Compile the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span> <span class="o">=</span> <span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="pytorch-extensions">
<h2><a class="toc-backref" href="#id143">PyTorch Extensions</a><a class="headerlink" href="#pytorch-extensions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="torch-aria2">
<span id="id1"></span><h3>Torch.aria2<a class="headerlink" href="#torch-aria2" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.aria2"></span><p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>See Aria paper:
<a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.aria2.Aria2">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.aria2.</code><code class="descname">Aria2</code><span class="sig-paren">(</span><em>beta=1.0</em>, <em>alpha=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/aria2.html#Aria2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.aria2.Aria2" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/aria2.png" src="_images/aria2.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id2"><span class="problematic" id="id3">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id4"><span class="problematic" id="id5">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyper-parameter which has a two-fold effect; it reduces the curvature in 3rd quadrant as well as increases the curvature in first quadrant while lowering the value of activation (default = 1)</li>
<li>beta: the exponential growth rate (default = 0.5)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See Aria paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Aria2</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.aria2.Aria2.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/aria2.html#Aria2.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.aria2.Aria2.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-mish">
<span id="id6"></span><h3>Torch.Mish<a class="headerlink" href="#torch-mish" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.mish"></span><p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<dl class="class">
<dt id="echoAI.Activation.Torch.mish.Mish">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.mish.</code><code class="descname">Mish</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/mish.html#Mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.mish.Mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mish.png" src="_images/mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id7"><span class="problematic" id="id8">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id9"><span class="problematic" id="id10">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>inplace: (bool) perform the operation in-place</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Mish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.mish.Mish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/mish.html#Mish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.mish.Mish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-echoAI.Activation.Torch.beta_mish">
<span id="torch-betamish"></span><span id="torch-beta-mish"></span><h3>Torch.BetaMish<a class="headerlink" href="#module-echoAI.Activation.Torch.beta_mish" title="Permalink to this headline">¶</a></h3>
<p>Applies the β mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<dl class="class">
<dt id="echoAI.Activation.Torch.beta_mish.BetaMish">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.beta_mish.</code><code class="descname">BetaMish</code><span class="sig-paren">(</span><em>beta=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/beta_mish.html#BetaMish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.beta_mish.BetaMish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the β mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/beta_mish.png" src="_images/beta_mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id11"><span class="problematic" id="id12">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id13"><span class="problematic" id="id14">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: hyperparameter (default = 1.5)</li>
</ul>
</dd>
<dt>References</dt>
<dd><ul class="first simple">
<li>β-Mish: An uni-parametric adaptive activation function derived from Mish:</li>
</ul>
<p class="last"><a class="reference external" href="https://github.com/digantamisra98/Beta-Mish">https://github.com/digantamisra98/Beta-Mish</a>)</p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">BetaMish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.beta_mish.BetaMish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/beta_mish.html#BetaMish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.beta_mish.BetaMish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-silu">
<span id="id15"></span><h3>Torch.Silu<a class="headerlink" href="#torch-silu" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.silu"></span><p>Applies the Sigmoid Linear Unit (SiLU) function element-wise:</p>
<div class="math notranslate nohighlight">
\[silu(x) = x * sigmoid(x)\]</div>
<p>See related paper:
<a class="reference external" href="https://arxiv.org/pdf/1606.08415.pdf">https://arxiv.org/pdf/1606.08415.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.silu.Silu">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.silu.</code><code class="descname">Silu</code><span class="sig-paren">(</span><em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/silu.html#Silu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.silu.Silu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Sigmoid Linear Unit (SiLU) function element-wise:</p>
<div class="math notranslate nohighlight">
\[silu(x) = x * sigmoid(x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/silu.png" src="_images/silu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id16"><span class="problematic" id="id17">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id18"><span class="problematic" id="id19">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>inplace - (bool) if inplace == True operation is performed inplace</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1606.08415.pdf">https://arxiv.org/pdf/1606.08415.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Silu</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.silu.Silu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/silu.html#Silu.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.silu.Silu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-eswish">
<span id="id20"></span><h3>Torch.Eswish<a class="headerlink" href="#torch-eswish" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.eswish"></span><p>Applies the E-Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>See E-Swish paper:
<a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.eswish.Eswish">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.eswish.</code><code class="descname">Eswish</code><span class="sig-paren">(</span><em>beta=1.75</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/eswish.html#Eswish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.eswish.Eswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the E-Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/eswish.png" src="_images/eswish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id21"><span class="problematic" id="id22">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id23"><span class="problematic" id="id24">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: a constant parameter (default value = 1.375)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Eswish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.375</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.eswish.Eswish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/eswish.html#Eswish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.eswish.Eswish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-swish">
<span id="id25"></span><h3>Torch.Swish<a class="headerlink" href="#torch-swish" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.swish"></span><p>Applies the Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>See Swish paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.swish.Swish">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.swish.</code><code class="descname">Swish</code><span class="sig-paren">(</span><em>beta=1.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/swish.html#Swish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.swish.Swish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/swish.png" src="_images/swish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id26"><span class="problematic" id="id27">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id28"><span class="problematic" id="id29">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: hyperparameter, which controls the shape of the bump (default = 1.25)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Swish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.25</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.swish.Swish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/swish.html#Swish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.swish.Swish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-elish">
<span id="id30"></span><h3>Torch.Elish<a class="headerlink" href="#torch-elish" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.elish"></span><p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See ELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1808.00783.pdf">https://arxiv.org/pdf/1808.00783.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.elish.Elish">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.elish.</code><code class="descname">Elish</code><a class="reference internal" href="_modules/echoAI/Activation/Torch/elish.html#Elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.elish.Elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/elish.png" src="_images/elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id31"><span class="problematic" id="id32">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id33"><span class="problematic" id="id34">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Elish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.elish.Elish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/elish.html#Elish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.elish.Elish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-echoAI.Activation.Torch.hard_elish">
<span id="torch-hardelish"></span><span id="torch-hard-elish"></span><h3>Torch.HardElish<a class="headerlink" href="#module-echoAI.Activation.Torch.hard_elish" title="Permalink to this headline">¶</a></h3>
<p>Applies the HardELiSH function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See HardELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1808.00783.pdf">https://arxiv.org/pdf/1808.00783.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.hard_elish.HardElish">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.hard_elish.</code><code class="descname">HardElish</code><a class="reference internal" href="_modules/echoAI/Activation/Torch/hard_elish.html#HardElish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.hard_elish.HardElish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardELiSH function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/hard_elish.png" src="_images/hard_elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id35"><span class="problematic" id="id36">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id37"><span class="problematic" id="id38">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See HardELiSH paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">HardElish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.hard_elish.HardElish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/hard_elish.html#HardElish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.hard_elish.HardElish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-mila">
<span id="id39"></span><h3>Torch.Mila<a class="headerlink" href="#torch-mila" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.mila"></span><p>Applies the Mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x))\]</div>
<p>Refer to:
<a class="reference external" href="https://github.com/digantamisra98/Mila">https://github.com/digantamisra98/Mila</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.mila.Mila">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.mila.</code><code class="descname">Mila</code><span class="sig-paren">(</span><em>beta=-0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/mila.html#Mila"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.mila.Mila" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mila.png" src="_images/mila.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id40"><span class="problematic" id="id41">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id42"><span class="problematic" id="id43">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: scale to control the concavity of the global minima of the function (default = -0.25)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><a class="reference external" href="https://github.com/digantamisra98/Mila">https://github.com/digantamisra98/Mila</a></li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">Mila</span><span class="p">(</span><span class="n">beta</span><span class="o">=-</span><span class="mf">0.25</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.mila.Mila.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/mila.html#Mila.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.mila.Mila.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-echoAI.Activation.Torch.sine_relu">
<span id="torch-sinerelu"></span><span id="torch-sine-relu"></span><h3>Torch.SineReLU<a class="headerlink" href="#module-echoAI.Activation.Torch.sine_relu" title="Permalink to this headline">¶</a></h3>
<p>Applies the SineReLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>See related Medium article:
<a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.sine_relu.SineReLU">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.sine_relu.</code><code class="descname">SineReLU</code><span class="sig-paren">(</span><em>epsilon=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/sine_relu.html#SineReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.sine_relu.SineReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SineReLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sine_relu.png" src="_images/sine_relu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id44"><span class="problematic" id="id45">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id46"><span class="problematic" id="id47">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>epsilon: hyperparameter (default = 0.01) used to control the wave amplitude</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related Medium article:</li>
</ul>
<p class="last"><a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">SineReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.sine_relu.SineReLU.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/sine_relu.html#SineReLU.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.sine_relu.SineReLU.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-fts">
<span id="id48"></span><h3>Torch.FTS<a class="headerlink" href="#torch-fts" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.fts"></span><p>Applies the FTS (Flatten T-Swish) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See Flatten T-Swish paper:
<a class="reference external" href="https://arxiv.org/pdf/1812.06247.pdf">https://arxiv.org/pdf/1812.06247.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.fts.FTS">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.fts.</code><code class="descname">FTS</code><a class="reference internal" href="_modules/echoAI/Activation/Torch/fts.html#FTS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.fts.FTS" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the FTS (Flatten T-Swish) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/fts.png" src="_images/fts.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id49"><span class="problematic" id="id50">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id51"><span class="problematic" id="id52">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Flattened T-Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1812.06247.pdf">https://arxiv.org/pdf/1812.06247.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">FTS</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.fts.FTS.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/fts.html#FTS.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.fts.FTS.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-sqnl">
<span id="id53"></span><h3>Torch.SQNL<a class="headerlink" href="#torch-sqnl" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.sqnl"></span><p>Applies the SQNL function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>See SQNL paper:
<a class="reference external" href="https://ieeexplore.ieee.org/document/8489043">https://ieeexplore.ieee.org/document/8489043</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.sqnl.SQNL">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.sqnl.</code><code class="descname">SQNL</code><a class="reference internal" href="_modules/echoAI/Activation/Torch/sqnl.html#SQNL"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.sqnl.SQNL" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SQNL function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sqnl.png" src="_images/sqnl.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id54"><span class="problematic" id="id55">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id56"><span class="problematic" id="id57">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See SQNL paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://ieeexplore.ieee.org/document/8489043">https://ieeexplore.ieee.org/document/8489043</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">SQNL</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.sqnl.SQNL.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/sqnl.html#SQNL.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.sqnl.SQNL.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-isru">
<span id="id58"></span><h3>Torch.ISRU<a class="headerlink" href="#torch-isru" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.isru"></span><p>Applies the ISRU (Inverse Square Root Unit) function element-wise:</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>ISRU paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.isru.ISRU">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.isru.</code><code class="descname">ISRU</code><span class="sig-paren">(</span><em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/isru.html#ISRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.isru.ISRU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRU function element-wise:</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isru.png" src="_images/isru.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id59"><span class="problematic" id="id60">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id61"><span class="problematic" id="id62">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: A constant (default = 1.0)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>ISRU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">ISRU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.isru.ISRU.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/isru.html#ISRU.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.isru.ISRU.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-isrlu">
<span id="id63"></span><h3>Torch.ISRLU<a class="headerlink" href="#torch-isrlu" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.isrlu"></span><p>Applies the ISRLU (Inverse Square Root Linear Unit) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>ISRLU paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.isrlu.ISRLU">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.isrlu.</code><code class="descname">ISRLU</code><span class="sig-paren">(</span><em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/isrlu.html#ISRLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.isrlu.ISRLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isrlu.png" src="_images/isrlu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id64"><span class="problematic" id="id65">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id66"><span class="problematic" id="id67">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyperparameter α controls the value to which an ISRLU saturates for negative inputs (default = 1)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li>ISRLU paper: <a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">ISRLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.isrlu.ISRLU.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/isrlu.html#ISRLU.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.isrlu.ISRLU.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-echoAI.Activation.Torch.bent_id">
<span id="torch-bentid"></span><span id="torch-bent-id"></span><h3>Torch.BentID<a class="headerlink" href="#module-echoAI.Activation.Torch.bent_id" title="Permalink to this headline">¶</a></h3>
<p>Applies the Bent’s Identity function element-wise:</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<dl class="class">
<dt id="echoAI.Activation.Torch.bent_id.BentID">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.bent_id.</code><code class="descname">BentID</code><a class="reference internal" href="_modules/echoAI/Activation/Torch/bent_id.html#BentID"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.bent_id.BentID" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Bent’s Identity function element-wise:</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/bent_id.png" src="_images/bent_id.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id68"><span class="problematic" id="id69">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id70"><span class="problematic" id="id71">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">BentID</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.bent_id.BentID.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/bent_id.html#BentID.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.bent_id.BentID.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-echoAI.Activation.Torch.soft_clipping">
<span id="torch-softclipping"></span><span id="torch-soft-clipping"></span><h3>Torch.SoftClipping<a class="headerlink" href="#module-echoAI.Activation.Torch.soft_clipping" title="Permalink to this headline">¶</a></h3>
<p>Applies Soft Clipping function element-wise:</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>See SC paper:
<a class="reference external" href="https://arxiv.org/pdf/1810.11509.pdf">https://arxiv.org/pdf/1810.11509.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.soft_clipping.SoftClipping">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.soft_clipping.</code><code class="descname">SoftClipping</code><span class="sig-paren">(</span><em>alpha=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/soft_clipping.html#SoftClipping"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.soft_clipping.SoftClipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Soft Clipping function element-wise:</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sc.png" src="_images/sc.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id72"><span class="problematic" id="id73">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id74"><span class="problematic" id="id75">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyper-parameter, which determines how close to linear the central region is and how sharply the linear region turns to the asymptotic values</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See SC paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/pdf/1810.11509.pdf">https://arxiv.org/pdf/1810.11509.pdf</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">SoftClipping</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.soft_clipping.SoftClipping.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/soft_clipping.html#SoftClipping.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.soft_clipping.SoftClipping.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-weightedtanh">
<span id="id76"></span><h3>Torch.WeightedTanh<a class="headerlink" href="#torch-weightedtanh" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.weightedTanh"></span><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<dl class="class">
<dt id="echoAI.Activation.Torch.weightedTanh.WeightedTanh">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.weightedTanh.</code><code class="descname">WeightedTanh</code><span class="sig-paren">(</span><em>weight=1</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/weightedTanh.html#WeightedTanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.weightedTanh.WeightedTanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/weighted_tanh.png" src="_images/weighted_tanh.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id77"><span class="problematic" id="id78">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id79"><span class="problematic" id="id80">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>weight: hyperparameter (default = 1.0)</li>
<li>inplace: perform inplace operation (default = False)</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">WeightedTanh</span><span class="p">(</span><span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.weightedTanh.WeightedTanh.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/weightedTanh.html#WeightedTanh.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.weightedTanh.WeightedTanh.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-srelu">
<span id="id81"></span><h3>Torch.SReLU<a class="headerlink" href="#torch-srelu" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.srelu"></span><p>Script defined the SReLU (S-shaped Rectified Linear Activation Unit):</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x_i) = \left\{\begin{matrix} t_i^r + a_i^r(x_i - t_i^r), x_i \geq t_i^r \\  x_i, t_i^r &gt; x_i &gt; t_i^l\\  t_i^l + a_i^l(x_i - t_i^l), x_i \leq  t_i^l \\ \end{matrix}\right.\end{split}\]</div>
<p>See SReLU paper:
<a class="reference external" href="https://arxiv.org/pdf/1512.07030.pdf">https://arxiv.org/pdf/1512.07030.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.srelu.SReLU">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.srelu.</code><code class="descname">SReLU</code><span class="sig-paren">(</span><em>in_features</em>, <em>parameters=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/srelu.html#SReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.srelu.SReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>SReLU (S-shaped Rectified Linear Activation Unit): a combination of three linear functions, which perform mapping R → R with the following formulation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x_i) = \left\{\begin{matrix} t_i^r + a_i^r(x_i - t_i^r), x_i \geq t_i^r \\  x_i, t_i^r &gt; x_i &gt; t_i^l\\  t_i^l + a_i^l(x_i - t_i^l), x_i \leq  t_i^l \\ \end{matrix}\right.\end{split}\]</div>
<p>with 4 trainable parameters.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id82"><span class="problematic" id="id83">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id84"><span class="problematic" id="id85">*</span></a>), same shape as the input</li>
</ul>
</dd>
</dl>
<p>Parameters:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\{t_i^r, a_i^r, t_i^l, a_i^l\}\]</div>
</div></blockquote>
<p>4 trainable parameters, which model an individual SReLU activation unit. The subscript i indicates that we allow SReLU to vary in different channels. Parameters can be initialized manually or randomly.</p>
<dl class="docutils">
<dt>References:</dt>
<dd><ul class="first simple">
<li>See SReLU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1512.07030.pdf">https://arxiv.org/pdf/1512.07030.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">srelu_activation</span> <span class="o">=</span> <span class="n">srelu</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">srelu_activation</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.srelu.SReLU.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/srelu.html#SReLU.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.srelu.SReLU.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-brelu">
<span id="id86"></span><h3>Torch.BReLU<a class="headerlink" href="#torch-brelu" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.brelu"></span><p>Script defined the BReLU (Bipolar Rectified Linear Activation Unit):</p>
<div class="math notranslate nohighlight">
\[\begin{split}BReLU(x_i) = \left\{\begin{matrix} f(x_i), i \mod 2 = 0\\  - f(-x_i), i \mod 2 \neq  0 \end{matrix}\right.\end{split}\]</div>
<p>See BReLU paper:
<a class="reference external" href="https://arxiv.org/pdf/1709.04054.pdf">https://arxiv.org/pdf/1709.04054.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.brelu.BReLU">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.brelu.</code><code class="descname">BReLU</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/brelu.html#BReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.brelu.BReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of BReLU activation function:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}BReLU(x_i) = \left\{\begin{matrix} f(x_i), i \mod 2 = 0\\  - f(-x_i), i \mod 2 \neq  0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/brelu.png" src="_images/brelu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id87"><span class="problematic" id="id88">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id89"><span class="problematic" id="id90">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See BReLU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1709.04054.pdf">https://arxiv.org/pdf/1709.04054.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">brelu_activation</span> <span class="o">=</span> <span class="n">brelu</span><span class="o">.</span><span class="n">apply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">brelu_activation</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="staticmethod">
<dt id="echoAI.Activation.Torch.brelu.BReLU.backward">
<em class="property">static </em><code class="descname">backward</code><span class="sig-paren">(</span><em>ctx</em>, <em>grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/brelu.html#BReLU.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.brelu.BReLU.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the backward pass we receive a Tensor containing the gradient of the loss
with respect to the output, and we need to compute the gradient of the loss
with respect to the input.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="echoAI.Activation.Torch.brelu.BReLU.forward">
<em class="property">static </em><code class="descname">forward</code><span class="sig-paren">(</span><em>ctx</em>, <em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/brelu.html#BReLU.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.brelu.BReLU.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the forward pass we receive a Tensor containing the input and return
a Tensor containing the output. ctx is a context object that can be used
to stash information for backward computation. You can cache arbitrary
objects for use in the backward pass using the ctx.save_for_backward method.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-apl">
<span id="id91"></span><h3>Torch.APL<a class="headerlink" href="#torch-apl" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.apl"></span><p>Script defined the APL (ADAPTIVE PIECEWISE LINEAR UNITS):</p>
<div class="math notranslate nohighlight">
\[APL(x_i) = max(0,x) + \sum_{s=1}^{S}{a_i^s * max(0, -x + b_i^s)}\]</div>
<p>See APL paper:
<a class="reference external" href="https://arxiv.org/pdf/1412.6830.pdf">https://arxiv.org/pdf/1412.6830.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.apl.APL">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.apl.</code><code class="descname">APL</code><span class="sig-paren">(</span><em>in_features</em>, <em>S</em>, <em>a=None</em>, <em>b=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/apl.html#APL"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.apl.APL" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of APL (ADAPTIVE PIECEWISE LINEAR UNITS) unit:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[APL(x_i) = max(0,x) + \sum_{s=1}^{S}{a_i^s * max(0, -x + b_i^s)}\]</div>
</div></blockquote>
<p>with trainable parameters a and b, parameter S should be set in advance.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id92"><span class="problematic" id="id93">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id94"><span class="problematic" id="id95">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Parameters:</dt>
<dd><ul class="first last simple">
<li>S: hyperparameter, number of hinges to be set in advance</li>
<li>a: trainable parameter, control the slopes of the linear segments</li>
<li>b: trainable parameter, determine the locations of the hinges</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See APL paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1412.6830.pdf">https://arxiv.org/pdf/1412.6830.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a1</span> <span class="o">=</span> <span class="n">apl</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">a1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.apl.APL.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/apl.html#APL.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.apl.APL.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="echoAI.Activation.Torch.apl.apl_function">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.apl.</code><code class="descname">apl_function</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/apl.html#apl_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.apl.apl_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of APL (ADAPTIVE PIECEWISE LINEAR UNITS) activation function:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[APL(x_i) = max(0,x) + \sum_{s=1}^{S}{a_i^s * max(0, -x + b_i^s)}\]</div>
</div></blockquote>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id96"><span class="problematic" id="id97">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id98"><span class="problematic" id="id99">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>a: variables control the slopes of the linear segments</li>
<li>b: variables determine the locations of the hinges</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See APL paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1412.6830.pdf">https://arxiv.org/pdf/1412.6830.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">apl_func</span> <span class="o">=</span> <span class="n">apl_function</span><span class="o">.</span><span class="n">apply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]],[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]],[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">apl_func</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="staticmethod">
<dt id="echoAI.Activation.Torch.apl.apl_function.backward">
<em class="property">static </em><code class="descname">backward</code><span class="sig-paren">(</span><em>ctx</em>, <em>grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/apl.html#apl_function.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.apl.apl_function.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the backward pass we receive a Tensor containing the gradient of the loss
with respect to the output, and we need to compute the gradient of the loss
with respect to the input.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="echoAI.Activation.Torch.apl.apl_function.forward">
<em class="property">static </em><code class="descname">forward</code><span class="sig-paren">(</span><em>ctx</em>, <em>input</em>, <em>a</em>, <em>b</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/apl.html#apl_function.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.apl.apl_function.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the forward pass we receive a Tensor containing the input and return
a Tensor containing the output. ctx is a context object that can be used
to stash information for backward computation. You can cache arbitrary
objects for use in the backward pass using the ctx.save_for_backward method.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-echoAI.Activation.Torch.soft_exponential">
<span id="torch-softexponential"></span><span id="torch-soft-exponential"></span><h3>Torch.SoftExponential<a class="headerlink" href="#module-echoAI.Activation.Torch.soft_exponential" title="Permalink to this headline">¶</a></h3>
<p>Script implements soft exponential activation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SoftExponential(x, \alpha) = \left\{\begin{matrix} - \frac{log(1 - \alpha(x + \alpha))}{\alpha}, \alpha &lt; 0\\  x, \alpha = 0\\  \frac{e^{\alpha * x} - 1}{\alpha} + \alpha, \alpha &gt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See related paper:
<a class="reference external" href="https://arxiv.org/pdf/1602.01321.pdf">https://arxiv.org/pdf/1602.01321.pdf</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.soft_exponential.SoftExponential">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.soft_exponential.</code><code class="descname">SoftExponential</code><span class="sig-paren">(</span><em>in_features</em>, <em>alpha=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/soft_exponential.html#SoftExponential"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.soft_exponential.SoftExponential" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of soft exponential activation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}SoftExponential(x, \alpha) = \left\{\begin{matrix} - \frac{log(1 - \alpha(x + \alpha))}{\alpha}, \alpha &lt; 0\\  x, \alpha = 0\\  \frac{e^{\alpha * x} - 1}{\alpha} + \alpha, \alpha &gt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>with trainable parameter alpha.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id100"><span class="problematic" id="id101">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id102"><span class="problematic" id="id103">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Parameters:</dt>
<dd><ul class="first last simple">
<li>alpha - trainable parameter</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1602.01321.pdf">https://arxiv.org/pdf/1602.01321.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a1</span> <span class="o">=</span> <span class="n">soft_exponential</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">a1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="echoAI.Activation.Torch.soft_exponential.SoftExponential.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/soft_exponential.html#SoftExponential.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.soft_exponential.SoftExponential.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="torch-maxout">
<span id="id104"></span><h3>Torch.Maxout<a class="headerlink" href="#torch-maxout" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.maxout"></span><p>Script implements Maxout activation</p>
<div class="math notranslate nohighlight">
\[maxout(\vec{x}) = max_i(x_i)\]</div>
<p>See related paper:
<a class="reference external" href="https://arxiv.org/pdf/1302.4389.pdf">https://arxiv.org/pdf/1302.4389.pdf</a></p>
<p>See implementation:
<a class="reference external" href="https://github.com/Usama113/Maxout-PyTorch/blob/master/Maxout.ipynb">https://github.com/Usama113/Maxout-PyTorch/blob/master/Maxout.ipynb</a></p>
<dl class="class">
<dt id="echoAI.Activation.Torch.maxout.Maxout">
<em class="property">class </em><code class="descclassname">echoAI.Activation.Torch.maxout.</code><code class="descname">Maxout</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/maxout.html#Maxout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.maxout.Maxout" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of Maxout:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[maxout(\vec{x}) = max_i(x_i)\]</div>
</div></blockquote>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id105"><span class="problematic" id="id106">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id107"><span class="problematic" id="id108">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Maxout paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1302.4389.pdf">https://arxiv.org/pdf/1302.4389.pdf</a></p>
<ul class="simple">
<li>Reference to the implementation:</li>
</ul>
<p class="last"><a class="reference external" href="https://github.com/Usama113/Maxout-PyTorch/blob/master/Maxout.ipynb">https://github.com/Usama113/Maxout-PyTorch/blob/master/Maxout.ipynb</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a1</span> <span class="o">=</span> <span class="n">maxout</span><span class="o">.</span><span class="n">apply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">a1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="echoai-activation-torch-functional">
<h3>echoAI.Activation.Torch.functional<a class="headerlink" href="#echoai-activation-torch-functional" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Torch.functional"></span><p>Script provides functional interface for custom activation functions.</p>
<dl class="function">
<dt id="echoAI.Activation.Torch.functional.aria2">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">aria2</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>alpha=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#aria2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.aria2" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.aria2" title="echoAI.Activation.Torch.aria2"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.aria2</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.bent_id">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">bent_id</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#bent_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.bent_id" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Bent’s Identity function element-wise:</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.bent_id" title="echoAI.Activation.Torch.bent_id"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.bent_id</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.beta_mish">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">beta_mish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#beta_mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.beta_mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the β mish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.beta_mish" title="echoAI.Activation.Torch.beta_mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.beta_mish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.elish">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">elish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.elish" title="echoAI.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.elish</span></code></a>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.elish" title="echoAI.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.elish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.eswish">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">eswish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.75</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#eswish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.eswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the E-Swish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.eswish" title="echoAI.Activation.Torch.eswish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.eswish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.fts">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">fts</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#fts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.fts" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the FTS (Flatten T-Swish) activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.fts" title="echoAI.Activation.Torch.fts"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.fts</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.hard_elish">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">hard_elish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#hard_elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.hard_elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.hard_elish" title="echoAI.Activation.Torch.hard_elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.hard_elish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.isrlu">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">isrlu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#isrlu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.isrlu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.isrlu" title="echoAI.Activation.Torch.isrlu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.isrlu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.isru">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">isru</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#isru"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.isru" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRU function element-wise:</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.isru" title="echoAI.Activation.Torch.isru"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.isru</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.mila">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">mila</code><span class="sig-paren">(</span><em>input</em>, <em>beta=-0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#mila"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.mila" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(softplus(\beta + x)) = x * tanh(ln(1 + e^{\beta + x}))\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.mila" title="echoAI.Activation.Torch.mila"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.mila</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.mish">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">mish</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>See additional documentation for <code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAIAI.Activation.Torch.mish</span></code>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.silu">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">silu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#silu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.silu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Sigmoid Linear Unit (SiLU) function element-wise:</p>
<div class="math notranslate nohighlight">
\[SiLU(x) = x * sigmoid(x)\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.silu" title="echoAI.Activation.Torch.silu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.silu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.sineReLU">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">sineReLU</code><span class="sig-paren">(</span><em>input</em>, <em>eps=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#sineReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.sineReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SineReLU activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.sine_relu" title="echoAI.Activation.Torch.sine_relu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.sine_relu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.soft_clipping">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">soft_clipping</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#soft_clipping"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.soft_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Soft Clipping function element-wise:</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.soft_clipping" title="echoAI.Activation.Torch.soft_clipping"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.soft_clipping</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.sqnl">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">sqnl</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#sqnl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.sqnl" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SQNL activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.sqnl" title="echoAI.Activation.Torch.sqnl"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.sqnl</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.swish">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">swish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#swish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.swish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Swish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.swish" title="echoAI.Activation.Torch.swish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.swish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="echoAI.Activation.Torch.functional.weighted_tanh">
<code class="descclassname">echoAI.Activation.Torch.functional.</code><code class="descname">weighted_tanh</code><span class="sig-paren">(</span><em>input</em>, <em>weight=1</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/echoAI/Activation/Torch/functional.html#weighted_tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#echoAI.Activation.Torch.functional.weighted_tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-echoAI.Activation.Torch.weightedTanh" title="echoAI.Activation.Torch.weightedTanh"><code class="xref py py-mod docutils literal notranslate"><span class="pre">echoAI.Activation.Torch.weightedTanh</span></code></a>.</p>
</dd></dl>

</div>
</div>
<div class="section" id="keras-extensions">
<h2><a class="toc-backref" href="#id144">Keras Extensions</a><a class="headerlink" href="#keras-extensions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="keras-mish">
<span id="id109"></span><h3>Keras.Mish<a class="headerlink" href="#keras-mish" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.Mish"></span><p>Mish Activation Function.</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mish.png" src="_images/mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Mish</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.WeightedTanh">
<span id="keras-weightedtanh"></span><span id="keras-weighted-tanh"></span><h3>Keras.WeightedTanh<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.WeightedTanh" title="Permalink to this headline">¶</a></h3>
<p>Weighted TanH Activation Function.</p>
<div class="math notranslate nohighlight">
\[Weighted TanH(x, weight) = tanh(x * weight)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/weighted_tanh.png" src="_images/weighted_tanh.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>weight: hyperparameter (default=1.0)</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">WeightedTanh</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-aria2">
<span id="id110"></span><h3>Keras.Aria2<a class="headerlink" href="#keras-aria2" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.Aria2"></span><p>Aria-2 Activation Function.</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/aria2.png" src="_images/aria2.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyper-parameter which has a two-fold effect; it reduces the curvature in 3rd quadrant as well as increases the curvature in first quadrant while lowering the value of activation (default = 1)</li>
<li>beta: the exponential growth rate (default = 0.5)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See Aria paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span><span class="n">Aria2</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-eswish">
<span id="id111"></span><h3>Keras.Eswish<a class="headerlink" href="#keras-eswish" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.Eswish"></span><p>E-Swish Activation Function.</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/eswish.png" src="_images/eswish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: a constant parameter (default value = 1.375)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Eswish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-swish">
<span id="id112"></span><h3>Keras.Swish<a class="headerlink" href="#keras-swish" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.Swish"></span><p>Swish Activation Function.</p>
<div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/swish.png" src="_images/swish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>a constant or a trainable parameter (default=1; which is equivalent to  Sigmoid-weighted Linear Unit (SiL))</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Swish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-elish">
<span id="id113"></span><h3>Keras.Elish<a class="headerlink" href="#keras-elish" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.Elish"></span><p>ELiSH (Exponential Linear Sigmoid SquasHing) Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/elish.png" src="_images/elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Elish</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.HardElish">
<span id="keras-hardelish"></span><span id="keras-hard-elish"></span><h3>Keras.HardElish<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.HardElish" title="Permalink to this headline">¶</a></h3>
<p>Hard ELiSH Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/hard_elish.png" src="_images/hard_elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">HardElish</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-mila">
<span id="id114"></span><h3>Keras.Mila<a class="headerlink" href="#keras-mila" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.Mila"></span><p>Mila Activation Function.</p>
<div class="math notranslate nohighlight">
\[Mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mila.png" src="_images/mila.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: scale to control the concavity of the global minima of the function (default = -0.25)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><a class="reference external" href="https://github.com/digantamisra98/Mila">https://github.com/digantamisra98/Mila</a></li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Mila</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-sinerelu">
<span id="id115"></span><h3>Keras.SineReLU<a class="headerlink" href="#keras-sinerelu" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.SineReLU"></span><p>Sine ReLU Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x)-cos(x)), x \leq 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sine_relu.png" src="_images/sine_relu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related Medium article:</li>
</ul>
<p class="last"><a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>epsilon: hyperparameter (default=0.01)</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">SineReLU</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-fts">
<span id="id116"></span><h3>Keras.FTS<a class="headerlink" href="#keras-fts" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.FTS"></span><p>FTS (Flatten T-Swish) Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/fts.png" src="_images/fts.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Flatten T-Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1812.06247.pdf">https://arxiv.org/pdf/1812.06247.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">FTS</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-sqnl">
<span id="id117"></span><h3>Keras.SQNL<a class="headerlink" href="#keras-sqnl" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.SQNL"></span><p>SQNL Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sqnl.png" src="_images/sqnl.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>SQNL Paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://ieeexplore.ieee.org/document/8489043">https://ieeexplore.ieee.org/document/8489043</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">SQNL</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.BetaMish">
<span id="keras-betamish"></span><span id="keras-beta-mish"></span><h3>Keras.BetaMish<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.BetaMish" title="Permalink to this headline">¶</a></h3>
<p>β mish activation function.</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/beta_mish.png" src="_images/beta_mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: A constant or a trainable parameter (default = 1.5)</li>
</ul>
</dd>
<dt>References</dt>
<dd><ul class="first simple">
<li>β-Mish: An uni-parametric adaptive activation function derived from Mish:</li>
</ul>
<p class="last"><a class="reference external" href="https://github.com/digantamisra98/Beta-Mish">https://github.com/digantamisra98/Beta-Mish</a>)</p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">BetaMish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-isru">
<span id="id118"></span><h3>Keras.ISRU<a class="headerlink" href="#keras-isru" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.ISRU"></span><p>ISRU (Inverse Square Root Unit) Activation Function.</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isru.png" src="_images/isru.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: A constant (default = 1.0)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>ISRU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">ISRU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-isrlu">
<span id="id119"></span><h3>Keras.ISRLU<a class="headerlink" href="#keras-isrlu" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.ISRLU"></span><p>ISRLU Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isrlu.png" src="_images/isrlu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyperparameter α controls the value to which an ISRLU saturates for negative inputs (default = 1)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li>ISRLU paper: <a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">ISRLU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.BentID">
<span id="keras-bentid"></span><span id="keras-bent-id"></span><h3>Keras.BentID<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.BentID" title="Permalink to this headline">¶</a></h3>
<p>Bent’s Identity Activation Function.</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/bent_id.png" src="_images/bent_id.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">BentID</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.SoftClipping">
<span id="keras-softclipping"></span><span id="keras-soft-clipping"></span><h3>Keras.SoftClipping<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.SoftClipping" title="Permalink to this headline">¶</a></h3>
<p>Soft Clipping Activation Function.</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sc.png" src="_images/sc.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyper-parameter, which determines how close to linear the central region is and how sharply the linear region turns to the asymptotic values</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See SC paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/pdf/1810.11509.pdf">https://arxiv.org/pdf/1810.11509.pdf</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">SoftClipping</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-celu">
<span id="id120"></span><h3>Keras.Celu<a class="headerlink" href="#keras-celu" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.Celu"></span><p>CELU Activation Function.</p>
<div class="math notranslate nohighlight">
\[CELU(x, \alpha) = max(0,x) + min(0,\alpha * (exp(x/ \alpha)-1))\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: the α value for the CELU formulation (default=1.0)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See CELU paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/abs/1704.07483">https://arxiv.org/abs/1704.07483</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Celu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-relu6">
<span id="id121"></span><h3>Keras.ReLU6<a class="headerlink" href="#keras-relu6" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.ReLU6"></span><p>RELU6 Activation Function.</p>
<div class="math notranslate nohighlight">
\[RELU6(x) = min(max(0,x),6)\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See RELU6 paper:</dt>
<dd><a class="reference external" href="http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf">http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">ReLU6</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.HardTanh">
<span id="keras-hardtanh"></span><span id="keras-hard-tanh"></span><h3>Keras.HardTanh<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.HardTanh" title="Permalink to this headline">¶</a></h3>
<p>Hard-TanH Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}Hard-TanH(x) = \left\{\begin{matrix} 1, x &gt; 1 \\   x , -1 \leq x \leq 1 \\ -1, x &lt;- 1 \end{matrix}\right.\end{split}\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">HardTanh</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.LogSigmoid">
<span id="keras-logsigmoid"></span><span id="keras-log-sigmoid"></span><h3>Keras.LogSigmoid<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.LogSigmoid" title="Permalink to this headline">¶</a></h3>
<p>Log-Sigmoid Activation Function.</p>
<div class="math notranslate nohighlight">
\[Log-Sigmoid(x) = log (\frac{1}{1+e^{-x}})\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">log_sigmoid</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.TanhShrink">
<span id="keras-tanhshrink"></span><span id="keras-tanh-shrink"></span><h3>Keras.TanhShrink<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.TanhShrink" title="Permalink to this headline">¶</a></h3>
<p>TanH-Shrink Activation Function.</p>
<div class="math notranslate nohighlight">
\[TanH-Shrink(x) = x - tanh(x)\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">TanhShrink</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.HardShrink">
<span id="keras-hardshrink"></span><span id="keras-hard-shrink"></span><h3>Keras.HardShrink<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.HardShrink" title="Permalink to this headline">¶</a></h3>
<p>Hard-Shrink Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}Hard-Shrink(x) = \left\{\begin{matrix} x, x &gt; \lambda \\   0 , - \lambda \leq x \leq \lambda \\ x, x &lt;- \lambda \end{matrix}\right.\end{split}\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>lambda: the λ value for the Hardshrink formulation (default=0.5)</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">HardShrink</span><span class="p">(</span><span class="n">lambd</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.SoftShrink">
<span id="keras-softshrink"></span><span id="keras-soft-shrink"></span><h3>Keras.SoftShrink<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.SoftShrink" title="Permalink to this headline">¶</a></h3>
<p>Soft-Shrink Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}Soft-Shrink(x) = \left\{\begin{matrix} x - \lambda , x &gt; \lambda \\   0 , - \lambda \leq x \leq \lambda \\ x + \lambda , x &lt;- \lambda \end{matrix}\right.\end{split}\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>lambda: the λ value for the Softshrink formulation (default=0.5)</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">SoftShrink</span><span class="p">(</span><span class="n">lambd</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-softmin">
<span id="id122"></span><h3>Keras.SoftMin<a class="headerlink" href="#keras-softmin" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.SoftMin"></span><p>SoftMin Activation Function.</p>
<div class="math notranslate nohighlight">
\[SoftMin(x) = Softmax(-x)\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">SoftMin</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.LogSoftmax">
<span id="keras-logsoftmax"></span><span id="keras-log-softmax"></span><h3>Keras.LogSoftmax<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.LogSoftmax" title="Permalink to this headline">¶</a></h3>
<p>Log-SoftMax Activation Function.</p>
<div class="math notranslate nohighlight">
\[Log-SoftMax(x) = log(Softmax(-x))\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">LogSoftmax</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="module-echoAI.Activation.Keras.custom_activations.SoftExponential">
<span id="keras-softexponential"></span><span id="keras-soft-exponential"></span><h3>Keras.SoftExponential<a class="headerlink" href="#module-echoAI.Activation.Keras.custom_activations.SoftExponential" title="Permalink to this headline">¶</a></h3>
<p>Soft-Exponential Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}SoftExponential(x, \alpha) = \left\{\begin{matrix} - \frac{log(1 - \alpha(x + \alpha))}{\alpha}, \alpha &lt; 0\\  x, \alpha = 0\\  \frac{e^{\alpha * x} - 1}{\alpha} + \alpha, \alpha &gt; 0 \end{matrix}\right.\end{split}\]</div>
<p>with 1 trainable parameter.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
<dt>Parameters:</dt>
<dd><ul class="first last simple">
<li>alpha - trainable parameter</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Soft-Exponential paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1602.01321.pdf">https://arxiv.org/pdf/1602.01321.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">SoftExponential</span><span class="p">()(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keras-srelu">
<span id="id123"></span><h3>Keras.SReLU<a class="headerlink" href="#keras-srelu" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-echoAI.Activation.Keras.custom_activations.SReLU"></span><p>SReLU (S-shaped Rectified Linear Activation Unit): a combination of three linear functions, which perform mapping R → R with the following formulation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x_i) = \left\{\begin{matrix} t_i^r + a_i^r(x_i - t_i^r), x_i \geq t_i^r \\  x_i, t_i^r &gt; x_i &gt; t_i^l\\  t_i^l + a_i^l(x_i - t_i^l), x_i \leq  t_i^l \\ \end{matrix}\right.\end{split}\]</div>
<p>with 4 trainable parameters.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
</dl>
<p>Parameters:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\{t_i^r, a_i^r, t_i^l, a_i^l\}\]</div>
</div></blockquote>
<p>4 trainable parameters, which model an individual SReLU activation unit. The subscript i indicates that we allow SReLU to vary in different channels. Parameters can be initialized manually or randomly.</p>
<dl class="docutils">
<dt>References:</dt>
<dd><ul class="first simple">
<li>See SReLU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1512.07030.pdf">https://arxiv.org/pdf/1512.07030.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">SReLU</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">)(</span><span class="n">X_input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="tensorflow-keras-extensions">
<h2><a class="toc-backref" href="#id145">Tensorflow Keras Extensions</a><a class="headerlink" href="#tensorflow-keras-extensions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.Mish">
<span id="tensorflow-keras-mish"></span><span id="tf-keras-mish"></span><h3>Tensorflow_Keras.Mish<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.Mish" title="Permalink to this headline">¶</a></h3>
<p>Mish Activation Function.</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mish.png" src="_images/mish.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="first docutils">
<dt>See Mish Repository:</dt>
<dd><a class="reference external" href="https://github.com/digantamisra98/Mish">https://github.com/digantamisra98/Mish</a></dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.WeightedTanh">
<span id="tensorflow-keras-weightedtanh"></span><span id="tf-keras-weighted-tanh"></span><h3>Tensorflow_Keras.WeightedTanh<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.WeightedTanh" title="Permalink to this headline">¶</a></h3>
<p>Weighted TanH Activation Function.</p>
<div class="math notranslate nohighlight">
\[Weighted TanH(x, weight) = tanh(x * weight)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/weighted_tanh.png" src="_images/weighted_tanh.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>weight: hyperparameter (default=1.0)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.Swish">
<span id="tensorflow-keras-swish"></span><span id="tf-keras-swish"></span><h3>Tensorflow_Keras.Swish<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.Swish" title="Permalink to this headline">¶</a></h3>
<p>Swish Activation Function.</p>
<div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/swish.png" src="_images/swish.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>a constant or a trainable parameter (default=1; which is equivalent to  Sigmoid-weighted Linear Unit (SiL))</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>See Swish paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.ESwish">
<span id="tensorflow-keras-eswish"></span><span id="tf-keras-eswish"></span><h3>Tensorflow_Keras.ESwish<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.ESwish" title="Permalink to this headline">¶</a></h3>
<p>E-Swish Activation Function.</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/eswish.png" src="_images/eswish.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>beta: a constant parameter (default value = 1.375)</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>See related paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.Aria2">
<span id="tensorflow-keras-aria2"></span><span id="tf-keras-aria2"></span><h3>Tensorflow_Keras.Aria2<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.Aria2" title="Permalink to this headline">¶</a></h3>
<p>Aria-2 Activation Function.</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/aria2.png" src="_images/aria2.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>alpha: hyper-parameter which has a two-fold effect; it reduces the curvature in 3rd quadrant as well as increases the curvature in first quadrant while lowering the value of activation (default = 1)</li>
<li>beta: the exponential growth rate (default = 0.5)</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="first docutils">
<dt>See Aria paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.Mila">
<span id="tensorflow-keras-mila"></span><span id="tf-keras-mila"></span><h3>Tensorflow_Keras.Mila<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.Mila" title="Permalink to this headline">¶</a></h3>
<p>Mila Activation Function.</p>
<div class="math notranslate nohighlight">
\[Mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mila.png" src="_images/mila.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>beta: scale to control the concavity of the global minima of the function (default = -0.25)</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="https://github.com/digantamisra98/Mila">https://github.com/digantamisra98/Mila</a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.ISRU">
<span id="tensorflow-keras-isru"></span><span id="tf-keras-isru"></span><h3>Tensorflow_Keras.ISRU<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.ISRU" title="Permalink to this headline">¶</a></h3>
<p>ISRU (Inverse Square Root Unit) Activation Function.</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isru.png" src="_images/isru.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>alpha: A constant (default = 1.0)</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>ISRU paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.BentIdentity">
<span id="tensorflow-keras-bentidentity"></span><span id="tf-keras-bent-id"></span><h3>Tensorflow_Keras.BentIdentity<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.BentIdentity" title="Permalink to this headline">¶</a></h3>
<p>Bent’s Identity Activation Function.</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/bent_id.png" src="_images/bent_id.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.SoftClipping">
<span id="tensorflow-keras-softclipping"></span><span id="tf-keras-soft-clipping"></span><h3>Tensorflow_Keras.SoftClipping<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.SoftClipping" title="Permalink to this headline">¶</a></h3>
<p>Soft Clipping Activation Function.</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sc.png" src="_images/sc.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>alpha: hyper-parameter, which determines how close to linear the central region is and how sharply the linear region turns to the asymptotic values</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="first docutils">
<dt>See SC paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/pdf/1810.11509.pdf">https://arxiv.org/pdf/1810.11509.pdf</a></dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.BetaMish">
<span id="tensorflow-keras-betamish"></span><span id="tf-keras-beta-mish"></span><h3>Tensorflow_Keras.BetaMish<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.BetaMish" title="Permalink to this headline">¶</a></h3>
<p>β mish activation function.</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/beta_mish.png" src="_images/beta_mish.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>beta: A constant or a trainable parameter (default = 1.5)</li>
</ul>
</div></blockquote>
<p>References</p>
<blockquote>
<div><ul class="simple">
<li>β-Mish: An uni-parametric adaptive activation function derived from Mish:</li>
</ul>
<p><a class="reference external" href="https://github.com/digantamisra98/Beta-Mish">https://github.com/digantamisra98/Beta-Mish</a>)</p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.ELiSH">
<span id="tensorflow-keras-elish"></span><span id="tf-keras-elish"></span><h3>Tensorflow_Keras.ELiSH<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.ELiSH" title="Permalink to this headline">¶</a></h3>
<p>ELiSH (Exponential Linear Sigmoid SquasHing) Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/elish.png" src="_images/elish.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>Related paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.HardELiSH">
<span id="tensorflow-keras-hardelish"></span><span id="tf-keras-hard-elish"></span><h3>Tensorflow_Keras.HardELiSH<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.HardELiSH" title="Permalink to this headline">¶</a></h3>
<p>Hard ELiSH Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:
.. figure::  _static/hard_elish.png</p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">align:</th><td class="field-body">center</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>Related paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.SineReLU">
<span id="tensorflow-keras-sinerelu"></span><span id="tf-keras-sine-relu"></span><h3>Tensorflow_Keras.SineReLU<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.SineReLU" title="Permalink to this headline">¶</a></h3>
<p>Sine ReLU Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x)-cos(x)), x \leq 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:
.. figure::  _static/sine_relu.png</p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">align:</th><td class="field-body">center</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>See related Medium article:</li>
</ul>
<p><a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>epsilon: hyperparameter (default=0.01)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.FlattenTSwish">
<span id="tensorflow-keras-flattentswish"></span><span id="tf-keras-fts"></span><h3>Tensorflow_Keras.FlattenTSwish<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.FlattenTSwish" title="Permalink to this headline">¶</a></h3>
<p>FTS (Flatten T-Swish) Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:
.. figure::  _static/fts.png</p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">align:</th><td class="field-body">center</td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>Flatten T-Swish paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1812.06247.pdf">https://arxiv.org/pdf/1812.06247.pdf</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.SQNL">
<span id="tensorflow-keras-sqnl"></span><span id="tf-keras-sqnl"></span><h3>Tensorflow_Keras.SQNL<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.SQNL" title="Permalink to this headline">¶</a></h3>
<p>SQNL Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sqnl.png" src="_images/sqnl.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
</dl>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>SQNL Paper:</li>
</ul>
<p><a class="reference external" href="https://ieeexplore.ieee.org/document/8489043">https://ieeexplore.ieee.org/document/8489043</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.ISRLU">
<span id="tensorflow-keras-isrlu"></span><span id="tf-keras-isrlu"></span><h3>Tensorflow_Keras.ISRLU<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.ISRLU" title="Permalink to this headline">¶</a></h3>
<p>ISRLU Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isrlu.png" src="_images/isrlu.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>alpha: hyperparameter α controls the value to which an ISRLU saturates for negative inputs (default = 1)</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>ISRLU paper: <a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.SoftExponential">
<span id="tensorflow-keras-softexponential"></span><span id="tf-keras-soft-exponential"></span><h3>Tensorflow_Keras.SoftExponential<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.SoftExponential" title="Permalink to this headline">¶</a></h3>
<p>Soft-Exponential Activation Function with 1 trainable parameter..</p>
<div class="math notranslate nohighlight">
\[\begin{split}SoftExponential(x, \alpha) = \left\{\begin{matrix} - \frac{log(1 - \alpha(x + \alpha))}{\alpha}, \alpha &lt; 0\\  x, \alpha = 0\\  \frac{e^{\alpha * x} - 1}{\alpha} + \alpha, \alpha &gt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Parameters:</p>
<blockquote>
<div><ul class="simple">
<li>alpha - trainable parameter</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>See Soft-Exponential paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1602.01321.pdf">https://arxiv.org/pdf/1602.01321.pdf</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.CELU">
<span id="tensorflow-keras-celu"></span><span id="tf-keras-celu"></span><h3>Tensorflow_Keras.CELU<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.CELU" title="Permalink to this headline">¶</a></h3>
<p>CELU Activation Function.</p>
<div class="math notranslate nohighlight">
\[CELU(x, \alpha) = max(0,x) + min(0,\alpha * (exp(x/ \alpha)-1))\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
</dl>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>alpha: the α value for the CELU formulation (default=1.0)</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="first docutils">
<dt>See CELU paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/abs/1704.07483">https://arxiv.org/abs/1704.07483</a></dd>
</dl>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.HardTanh">
<span id="tensorflow-keras-hardtanh"></span><span id="tf-keras-hard-tanh"></span><h3>Tensorflow_Keras.HardTanh<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.HardTanh" title="Permalink to this headline">¶</a></h3>
<p>Hard-TanH Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}Hard-TanH(x) = \left\{\begin{matrix} 1, x &gt; 1 \\   x , -1 \leq x \leq 1 \\ -1, x &lt;- 1 \end{matrix}\right.\end{split}\]</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.LogSigmoid">
<span id="tensorflow-keras-logsigmoid"></span><span id="tf-keras-log-sigmoid"></span><h3>Tensorflow_Keras.LogSigmoid<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.LogSigmoid" title="Permalink to this headline">¶</a></h3>
<p>Log-Sigmoid Activation Function.</p>
<div class="math notranslate nohighlight">
\[Log-Sigmoid(x) = log (\frac{1}{1+e^{-x}})\]</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.TanhShrink">
<span id="tensorflow-keras-tanhshrink"></span><span id="tf-keras-tanh-shrink"></span><h3>Tensorflow_Keras.TanhShrink<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.TanhShrink" title="Permalink to this headline">¶</a></h3>
<p>TanH-Shrink Activation Function.</p>
<div class="math notranslate nohighlight">
\[TanH-Shrink(x) = x - tanh(x)\]</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.HardShrink">
<span id="tensorflow-keras-hardshrink"></span><span id="tf-keras-hard-shrink"></span><h3>Tensorflow_Keras.HardShrink<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.HardShrink" title="Permalink to this headline">¶</a></h3>
<p>Hard-Shrink Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}Hard-Shrink(x) = \left\{\begin{matrix} x, x &gt; \lambda \\   0 , - \lambda \leq x \leq \lambda \\ x, x &lt;- \lambda \end{matrix}\right.\end{split}\]</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>lambda: the λ value for the Hardshrink formulation (default=0.5)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.SoftShrink">
<span id="tensorflow-keras-softshrink"></span><span id="tf-keras-soft-shrink"></span><h3>Tensorflow_Keras.SoftShrink<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.SoftShrink" title="Permalink to this headline">¶</a></h3>
<p>Soft-Shrink Activation Function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}Soft-Shrink(x) = \left\{\begin{matrix} x - \lambda , x &gt; \lambda \\   0 , - \lambda \leq x \leq \lambda \\ x + \lambda , x &lt;- \lambda \end{matrix}\right.\end{split}\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="last simple">
<li>Output: Same shape as the input.</li>
</ul>
</dd>
</dl>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>lambda: the λ value for the Softshrink formulation (default=0.5)</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.SoftMin">
<span id="tensorflow-keras-softmin"></span><span id="tf-keras-softmin"></span><h3>Tensorflow_Keras.SoftMin<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.SoftMin" title="Permalink to this headline">¶</a></h3>
<p>SoftMin Activation Function.</p>
<div class="math notranslate nohighlight">
\[SoftMin(x) = Softmax(-x)\]</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.LogSoftMax">
<span id="tensorflow-keras-logsoftmax"></span><span id="tf-keras-log-softmax"></span><h3>Tensorflow_Keras.LogSoftMax<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.LogSoftMax" title="Permalink to this headline">¶</a></h3>
<p>Log-SoftMax Activation Function.</p>
<div class="math notranslate nohighlight">
\[Log-SoftMax(x) = log(Softmax(-x))\]</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: Arbitrary. Use the keyword argument <cite>input_shape</cite></li>
</ul>
<p>(tuple of integers, does not include the samples axis)
when using this layer as the first layer in a model.</p>
<ul class="simple">
<li>Output: Same shape as the input.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.MaxOut">
<span id="tensorflow-keras-maxout"></span><span id="tf-keras-maxout"></span><h3>Tensorflow_Keras.MaxOut<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.MaxOut" title="Permalink to this headline">¶</a></h3>
<p>Implementation of Maxout:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[maxout(\vec{x}) = max_i(x_i)\]</div>
</div></blockquote>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: (N, <a href="#id124"><span class="problematic" id="id125">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id126"><span class="problematic" id="id127">*</span></a>), same shape as the input</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>See Maxout paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1302.4389.pdf">https://arxiv.org/pdf/1302.4389.pdf</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.SReLU">
<span id="tensorflow-keras-srelu"></span><span id="tf-keras-srelu"></span><h3>Tensorflow_Keras.SReLU<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.SReLU" title="Permalink to this headline">¶</a></h3>
<p>SReLU (S-shaped Rectified Linear Activation Unit): a combination of three linear functions, which perform mapping R → R with the following formulation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x_i) = \left\{\begin{matrix} t_i^r + a_i^r(x_i - t_i^r), x_i \geq t_i^r \\  x_i, t_i^r &gt; x_i &gt; t_i^l\\  t_i^l + a_i^l(x_i - t_i^l), x_i \leq  t_i^l \\ \end{matrix}\right.\end{split}\]</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id128"><span class="problematic" id="id129">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id130"><span class="problematic" id="id131">*</span></a>), same shape as the input</li>
</ul>
</dd>
</dl>
<p>Parameters:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\{t_i^r, a_i^r, t_i^l, a_i^l\}\]</div>
</div></blockquote>
<p>4 trainable parameters, which model an individual SReLU activation unit. The subscript i indicates that we allow SReLU to vary in different channels. Parameters can be initialized manually or randomly.</p>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>See SReLU paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1512.07030.pdf">https://arxiv.org/pdf/1512.07030.pdf</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.BReLU">
<span id="tensorflow-keras-brelu"></span><span id="tf-keras-brelu"></span><h3>Tensorflow_Keras.BReLU<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.BReLU" title="Permalink to this headline">¶</a></h3>
<p>Implementation of BReLU activation function:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}BReLU(x_i) = \left\{\begin{matrix} f(x_i), i \mod 2 = 0\\  - f(-x_i), i \mod 2 \neq  0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/brelu.png" src="_images/brelu.png" />
</div>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: (N, <a href="#id132"><span class="problematic" id="id133">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id134"><span class="problematic" id="id135">*</span></a>), same shape as the input</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>See BReLU paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1709.04054.pdf">https://arxiv.org/pdf/1709.04054.pdf</a></p>
</div></blockquote>
</div>
<div class="section" id="module-echoAI.Activation.TF_Keras.custom_activation.APL">
<span id="tensorflow-keras-apl"></span><span id="tf-keras-apl"></span><h3>Tensorflow_Keras.APL<a class="headerlink" href="#module-echoAI.Activation.TF_Keras.custom_activation.APL" title="Permalink to this headline">¶</a></h3>
<p>Implementation of APL (ADAPTIVE PIECEWISE LINEAR UNITS) activation function:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[APL(x_i) = max(0,x) + \sum_{s=1}^{S}{a_i^s * max(0, -x + b_i^s)}\]</div>
</div></blockquote>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li>Input: (N, <a href="#id136"><span class="problematic" id="id137">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id138"><span class="problematic" id="id139">*</span></a>), same shape as the input</li>
</ul>
</div></blockquote>
<p>Arguments:</p>
<blockquote>
<div><ul class="simple">
<li>a: variables control the slopes of the linear segments</li>
<li>b: variables determine the locations of the hinges</li>
</ul>
</div></blockquote>
<p>References:</p>
<blockquote>
<div><ul class="simple">
<li>See APL paper:</li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1412.6830.pdf">https://arxiv.org/pdf/1412.6830.pdf</a></p>
</div></blockquote>
</div>
</div>
<div class="section" id="indices-and-tables">
<h2><a class="toc-backref" href="#id146">Indices and tables</a><a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="#">
    <img class="logo" src="_static/echo_logo.png" alt="Logo"/>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=digantamisra98&repo=Echo&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Diganta Misra, Aleksandra Deis.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>