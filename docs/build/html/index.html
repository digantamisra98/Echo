
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Welcome to Echo’s documentation! &#8212; Echo 2019 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="welcome-to-echo-s-documentation">
<h1>Welcome to Echo’s documentation!<a class="headerlink" href="#welcome-to-echo-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#about" id="id45">About</a><ul>
<li><a class="reference internal" href="#implemented-activation-functions" id="id46">Implemented Activation Functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#installation" id="id47">Installation</a></li>
<li><a class="reference internal" href="#torch-examples" id="id48">Torch Examples</a><ul>
<li><a class="reference internal" href="#activation-functions" id="id49">Activation Functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#echo-api-reference" id="id50">Echo API Reference</a><ul>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.aria2" id="id51">Echo.Activation.Torch.aria2</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.mish" id="id52">Echo.Activation.Torch.mish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.beta_mish" id="id53">Echo.Activation.Torch.beta_mish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.swish" id="id54">Echo.Activation.Torch.swish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.eswish" id="id55">Echo.Activation.Torch.eswish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.swishx" id="id56">Echo.Activation.Torch.swishx</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.elish" id="id57">Echo.Activation.Torch.elish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.hard_elish" id="id58">Echo.Activation.Torch.hard_elish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.mila" id="id59">Echo.Activation.Torch.mila</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.sine_relu" id="id60">Echo.Activation.Torch.sine_relu</a></li>
<li><a class="reference internal" href="#echo-activation-torch-weightedtanh" id="id61">Echo.Activation.Torch.weightedTanh</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.functional" id="id62">Echo.Activation.Torch.functional</a></li>
<li><a class="reference internal" href="#indices-and-tables" id="id63">Indices and tables</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="about">
<h2><a class="toc-backref" href="#id45">About</a><a class="headerlink" href="#about" title="Permalink to this headline">¶</a></h2>
<p><strong>Echo Package</strong> is created to provide an implementation of the most promising mathematical algorithms, which are missing in the most popular deep learning libraries, such as <a class="reference external" href="https://pytorch.org/">PyTorch</a>, <a class="reference external" href="https://keras.io/">Keras</a> and
<a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a>.</p>
<div class="section" id="implemented-activation-functions">
<h3><a class="toc-backref" href="#id46">Implemented Activation Functions</a><a class="headerlink" href="#implemented-activation-functions" title="Permalink to this headline">¶</a></h3>
<p>List of activation functions implemented in Echo:</p>
<ol class="arabic simple">
<li>PyTorch:</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Weighted Tanh (see <a class="reference internal" href="#module-Echo.Activation.Torch.weightedTanh" title="Echo.Activation.Torch.weightedTanh"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.weightedTanh</span></code></a>)</li>
<li>Aria2 (see <a class="reference internal" href="#module-Echo.Activation.Torch.aria2" title="Echo.Activation.Torch.aria2"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.aria2</span></code></a>)</li>
<li>Swish (see <a class="reference internal" href="#module-Echo.Activation.Torch.swish" title="Echo.Activation.Torch.swish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.swish</span></code></a>)</li>
<li>E-Swish (see <a class="reference internal" href="#module-Echo.Activation.Torch.eswish" title="Echo.Activation.Torch.eswish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.eswish</span></code></a>)</li>
<li>SwishX (see <a class="reference internal" href="#module-Echo.Activation.Torch.swishx" title="Echo.Activation.Torch.swishx"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.swishx</span></code></a>)</li>
<li>ELiSH (see <a class="reference internal" href="#module-Echo.Activation.Torch.elish" title="Echo.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.elish</span></code></a>)</li>
<li>Hard ELiSH (see <a class="reference internal" href="#module-Echo.Activation.Torch.hard_elish" title="Echo.Activation.Torch.hard_elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.hard_elish</span></code></a>)</li>
<li>Mila (see <a class="reference internal" href="#module-Echo.Activation.Torch.mila" title="Echo.Activation.Torch.mila"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mila</span></code></a>)</li>
<li>SineReLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.sine_relu" title="Echo.Activation.Torch.sine_relu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sine_relu</span></code></a>)</li>
<li>Mish (see <a class="reference internal" href="#module-Echo.Activation.Torch.mish" title="Echo.Activation.Torch.mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mish</span></code></a>)</li>
<li>Beta Mish (see <a class="reference internal" href="#module-Echo.Activation.Torch.beta_mish" title="Echo.Activation.Torch.beta_mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.beta_mish</span></code></a>)</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="installation">
<h2><a class="toc-backref" href="#id47">Installation</a><a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>To install Echo package follow the instructions below:</p>
<ol class="arabic simple">
<li>Clone or download <a class="reference external" href="https://github.com/digantamisra98/Echo">GitHub repository</a>.</li>
<li>Navigate to <strong>Echo</strong> folder:</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; $ cd Echo
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li>Install the package with pip:</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; $ pip install .
</pre></div>
</div>
</div>
<div class="section" id="torch-examples">
<h2><a class="toc-backref" href="#id48">Torch Examples</a><a class="headerlink" href="#torch-examples" title="Permalink to this headline">¶</a></h2>
<div class="section" id="activation-functions">
<h3><a class="toc-backref" href="#id49">Activation Functions</a><a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h3>
<p>The following code block contains an example of usage of an activation function
from Echo package:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># import activations from Echo</span>
<span class="hll"><span class="kn">from</span> <span class="nn">Echo.Activation.Torch.weightedTanh</span> <span class="kn">import</span> <span class="n">weightedTanh</span>
</span><span class="hll"><span class="kn">import</span> <span class="nn">Echo.Activation.Torch.functional</span> <span class="kn">as</span> <span class="nn">Func</span>
</span>
<span class="c1"># use activations in layers of model defined in class</span>
<span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># initialize layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># make sure the input tensor is flattened</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># apply activation function from Echo</span>
<span class="hll">        <span class="n">x</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">weighted_tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Initialize the model using defined Classifier class</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>

    <span class="c1"># Create model with Sequential</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
                         <span class="p">(</span><span class="s1">&#39;fc1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span>
                         <span class="c1"># use activation function from Echo</span>
<span class="hll">                         <span class="p">(</span><span class="s1">&#39;wtahn1&#39;</span><span class="p">,</span>  <span class="n">weightedTanh</span><span class="p">(</span><span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)),</span>
</span>                         <span class="p">(</span><span class="s1">&#39;fc2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;bn2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">128</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                         <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;fc3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;bn3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">64</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;relu3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                         <span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;logsoftmax&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="echo-api-reference">
<h2><a class="toc-backref" href="#id50">Echo API Reference</a><a class="headerlink" href="#echo-api-reference" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-Echo.Activation.Torch.aria2">
<span id="echo-activation-torch-aria2"></span><h3><a class="toc-backref" href="#id51">Echo.Activation.Torch.aria2</a><a class="headerlink" href="#module-Echo.Activation.Torch.aria2" title="Permalink to this headline">¶</a></h3>
<p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>See Aria paper:
<a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.aria2.aria2">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.aria2.</code><code class="descname">aria2</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/aria2.html#aria2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.aria2.aria2" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>Aria paper:
<a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></p>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/aria2.png" src="_images/aria2.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id1"><span class="problematic" id="id2">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id3"><span class="problematic" id="id4">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">aria2</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.aria2.aria2.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/aria2.html#aria2.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.aria2.aria2.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.mish">
<span id="echo-activation-torch-mish"></span><h3><a class="toc-backref" href="#id52">Echo.Activation.Torch.mish</a><a class="headerlink" href="#module-Echo.Activation.Torch.mish" title="Permalink to this headline">¶</a></h3>
<p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.mish.mish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.mish.</code><code class="descname">mish</code><a class="reference internal" href="_modules/Echo/Activation/Torch/mish.html#mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.mish.mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mish.png" src="_images/mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id5"><span class="problematic" id="id6">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id7"><span class="problematic" id="id8">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">mish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.mish.mish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/mish.html#mish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.mish.mish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.beta_mish">
<span id="echo-activation-torch-beta-mish"></span><h3><a class="toc-backref" href="#id53">Echo.Activation.Torch.beta_mish</a><a class="headerlink" href="#module-Echo.Activation.Torch.beta_mish" title="Permalink to this headline">¶</a></h3>
<p>Applies the β mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.beta_mish.beta_mish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.beta_mish.</code><code class="descname">beta_mish</code><span class="sig-paren">(</span><em>beta=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/beta_mish.html#beta_mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.beta_mish.beta_mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the β mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/beta_mish.png" src="_images/beta_mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id9"><span class="problematic" id="id10">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id11"><span class="problematic" id="id12">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">beta_mish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.beta_mish.beta_mish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/beta_mish.html#beta_mish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.beta_mish.beta_mish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.swish">
<span id="echo-activation-torch-swish"></span><h3><a class="toc-backref" href="#id54">Echo.Activation.Torch.swish</a><a class="headerlink" href="#module-Echo.Activation.Torch.swish" title="Permalink to this headline">¶</a></h3>
<p>Applies the swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[swish(x) = x * sigmoid(x)\]</div>
<p>See Swish paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.swish.swish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.swish.</code><code class="descname">swish</code><a class="reference internal" href="_modules/Echo/Activation/Torch/swish.html#swish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.swish.swish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[swish(x) = x * sigmoid(x)\]</div>
<p>Swish paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/swish.png" src="_images/swish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id13"><span class="problematic" id="id14">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id15"><span class="problematic" id="id16">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">swish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.swish.swish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/swish.html#swish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.swish.swish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.eswish">
<span id="echo-activation-torch-eswish"></span><h3><a class="toc-backref" href="#id55">Echo.Activation.Torch.eswish</a><a class="headerlink" href="#module-Echo.Activation.Torch.eswish" title="Permalink to this headline">¶</a></h3>
<p>Applies the E-Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>See E-Swish paper:
<a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.eswish.eswish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.eswish.</code><code class="descname">eswish</code><span class="sig-paren">(</span><em>beta=1.75</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/eswish.html#eswish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.eswish.eswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the E-Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>See E-Swish paper:
<a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/eswish.png" src="_images/eswish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id17"><span class="problematic" id="id18">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id19"><span class="problematic" id="id20">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">eswish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.375</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.eswish.eswish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/eswish.html#eswish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.eswish.eswish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.swishx">
<span id="echo-activation-torch-swishx"></span><h3><a class="toc-backref" href="#id56">Echo.Activation.Torch.swishx</a><a class="headerlink" href="#module-Echo.Activation.Torch.swishx" title="Permalink to this headline">¶</a></h3>
<p>Applies the Swish-X function element-wise:</p>
<div class="math notranslate nohighlight">
\[SwishX(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>See Swish-X paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.swishx.swishx">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.swishx.</code><code class="descname">swishx</code><span class="sig-paren">(</span><em>beta=1.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/swishx.html#swishx"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.swishx.swishx" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Swish-X function element-wise:</p>
<div class="math notranslate nohighlight">
\[SwishX(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>See Swish paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/swishx.png" src="_images/swishx.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id21"><span class="problematic" id="id22">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id23"><span class="problematic" id="id24">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">swishx</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.25</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.swishx.swishx.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/swishx.html#swishx.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.swishx.swishx.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.elish">
<span id="echo-activation-torch-elish"></span><h3><a class="toc-backref" href="#id57">Echo.Activation.Torch.elish</a><a class="headerlink" href="#module-Echo.Activation.Torch.elish" title="Permalink to this headline">¶</a></h3>
<p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See ELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1808.00783.pdf">https://arxiv.org/pdf/1808.00783.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.elish.elish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.elish.</code><code class="descname">elish</code><a class="reference internal" href="_modules/Echo/Activation/Torch/elish.html#elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.elish.elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See ELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/elish.png" src="_images/elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id25"><span class="problematic" id="id26">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id27"><span class="problematic" id="id28">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">elish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.elish.elish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/elish.html#elish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.elish.elish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.hard_elish">
<span id="echo-activation-torch-hard-elish"></span><h3><a class="toc-backref" href="#id58">Echo.Activation.Torch.hard_elish</a><a class="headerlink" href="#module-Echo.Activation.Torch.hard_elish" title="Permalink to this headline">¶</a></h3>
<p>Applies the HardELiSH function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See HardELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1808.00783.pdf">https://arxiv.org/pdf/1808.00783.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.hard_elish.hard_elish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.hard_elish.</code><code class="descname">hard_elish</code><a class="reference internal" href="_modules/Echo/Activation/Torch/hard_elish.html#hard_elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.hard_elish.hard_elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardELiSH function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See HardELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_static/hard_elish.png" src="_static/hard_elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id29"><span class="problematic" id="id30">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id31"><span class="problematic" id="id32">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">hard_elish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.hard_elish.hard_elish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/hard_elish.html#hard_elish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.hard_elish.hard_elish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.mila">
<span id="echo-activation-torch-mila"></span><h3><a class="toc-backref" href="#id59">Echo.Activation.Torch.mila</a><a class="headerlink" href="#module-Echo.Activation.Torch.mila" title="Permalink to this headline">¶</a></h3>
<p>Applies the Mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x))\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.mila.mila">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.mila.</code><code class="descname">mila</code><span class="sig-paren">(</span><em>beta=-0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/mila.html#mila"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.mila.mila" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mila.png" src="_images/mila.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id33"><span class="problematic" id="id34">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id35"><span class="problematic" id="id36">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">mila</span><span class="p">(</span><span class="n">beta</span><span class="o">=-</span><span class="mf">0.25</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.mila.mila.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/mila.html#mila.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.mila.mila.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.sine_relu">
<span id="echo-activation-torch-sine-relu"></span><h3><a class="toc-backref" href="#id60">Echo.Activation.Torch.sine_relu</a><a class="headerlink" href="#module-Echo.Activation.Torch.sine_relu" title="Permalink to this headline">¶</a></h3>
<p>Applies the SineReLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>See related Medium article:
<a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.sine_relu.sine_relu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.sine_relu.</code><code class="descname">sine_relu</code><a class="reference internal" href="_modules/Echo/Activation/Torch/sine_relu.html#sine_relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.sine_relu.sine_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SineReLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>See related Medium article:
<a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sine_relu.png" src="_images/sine_relu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id37"><span class="problematic" id="id38">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id39"><span class="problematic" id="id40">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">sine_relu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.sine_relu.sine_relu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/sine_relu.html#sine_relu.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.sine_relu.sine_relu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="echo-activation-torch-weightedtanh">
<h3><a class="toc-backref" href="#id61">Echo.Activation.Torch.weightedTanh</a><a class="headerlink" href="#echo-activation-torch-weightedtanh" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-Echo.Activation.Torch.weightedTanh"></span><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.weightedTanh.weightedTanh">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.weightedTanh.</code><code class="descname">weightedTanh</code><span class="sig-paren">(</span><em>weight=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/weightedTanh.html#weightedTanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.weightedTanh.weightedTanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/weighted_tanh.png" src="_images/weighted_tanh.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id41"><span class="problematic" id="id42">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id43"><span class="problematic" id="id44">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weightedTanh</span><span class="p">(</span><span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.weightedTanh.weightedTanh.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/weightedTanh.html#weightedTanh.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.weightedTanh.weightedTanh.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.functional">
<span id="echo-activation-torch-functional"></span><h3><a class="toc-backref" href="#id62">Echo.Activation.Torch.functional</a><a class="headerlink" href="#module-Echo.Activation.Torch.functional" title="Permalink to this headline">¶</a></h3>
<p>Script provides functional interface for custom activation functions.</p>
<dl class="function">
<dt id="Echo.Activation.Torch.functional.aria2">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">aria2</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>alpha=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#aria2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.aria2" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.aria2" title="Echo.Activation.Torch.aria2"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.aria2</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.beta_mish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">beta_mish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#beta_mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.beta_mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the β mish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.beta_mish" title="Echo.Activation.Torch.beta_mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.beta_mish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.elish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">elish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.elish" title="Echo.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.elish</span></code></a>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.elish" title="Echo.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.elish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.eswish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">eswish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.75</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#eswish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.eswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the E-Swish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.eswish" title="Echo.Activation.Torch.eswish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.eswish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.hard_elish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">hard_elish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#hard_elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.hard_elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.hard_elish" title="Echo.Activation.Torch.hard_elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.hard_elish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.mila">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">mila</code><span class="sig-paren">(</span><em>input</em>, <em>beta=-0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#mila"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.mila" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(softplus(\beta + x)) = x * tanh(ln(1 + e^{\beta + x}))\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.mila" title="Echo.Activation.Torch.mila"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mila</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.mish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">mish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.mish" title="Echo.Activation.Torch.mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.sineReLU">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">sineReLU</code><span class="sig-paren">(</span><em>input</em>, <em>eps=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#sineReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.sineReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SineReLU activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.sine_relu" title="Echo.Activation.Torch.sine_relu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sine_relu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.swish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">swish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#swish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.swish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[swish(x) = x * sigmoid(x)\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.swish" title="Echo.Activation.Torch.swish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.swish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.swishx">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">swishx</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#swishx"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.swishx" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Swish-X function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[SwishX(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.swishx" title="Echo.Activation.Torch.swishx"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.swishx</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.weighted_tanh">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">weighted_tanh</code><span class="sig-paren">(</span><em>input</em>, <em>weight=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#weighted_tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.weighted_tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.weightedTanh" title="Echo.Activation.Torch.weightedTanh"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.weightedTanh</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="indices-and-tables">
<h3><a class="toc-backref" href="#id63">Indices and tables</a><a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="#">
    <img class="logo" src="_static/echo_logo.png" alt="Logo"/>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=digantamisra98&repo=Echo&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Diganta Misra, Aleksandra Deis.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>