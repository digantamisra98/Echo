
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Welcome to Echo’s documentation! &#8212; Echo 2019 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="welcome-to-echo-s-documentation">
<h1>Welcome to Echo’s documentation!<a class="headerlink" href="#welcome-to-echo-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#about" id="id77">About</a><ul>
<li><a class="reference internal" href="#implemented-activation-functions" id="id78">Implemented Activation Functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#installation" id="id79">Installation</a></li>
<li><a class="reference internal" href="#torch-examples" id="id80">Torch Examples</a><ul>
<li><a class="reference internal" href="#activation-functions" id="id81">Activation Functions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#echo-api-reference" id="id82">Echo API Reference</a><ul>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.aria2" id="id83">Echo.Activation.Torch.aria2</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.mish" id="id84">Echo.Activation.Torch.mish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.beta_mish" id="id85">Echo.Activation.Torch.beta_mish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.silu" id="id86">Echo.Activation.Torch.silu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.eswish" id="id87">Echo.Activation.Torch.eswish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.swish" id="id88">Echo.Activation.Torch.swish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.elish" id="id89">Echo.Activation.Torch.elish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.hard_elish" id="id90">Echo.Activation.Torch.hard_elish</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.mila" id="id91">Echo.Activation.Torch.mila</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.sine_relu" id="id92">Echo.Activation.Torch.sine_relu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.fts" id="id93">Echo.Activation.Torch.fts</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.sqnl" id="id94">Echo.Activation.Torch.sqnl</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.isru" id="id95">Echo.Activation.Torch.isru</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.isrlu" id="id96">Echo.Activation.Torch.isrlu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.bent_id" id="id97">Echo.Activation.Torch.bent_id</a></li>
<li><a class="reference internal" href="#echo-activation-torch-soft-clipping" id="id98">Echo.Activation.Torch.soft_clipping</a></li>
<li><a class="reference internal" href="#echo-activation-torch-weightedtanh" id="id99">Echo.Activation.Torch.weightedTanh</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.srelu" id="id100">Echo.Activation.Torch.srelu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.brelu" id="id101">Echo.Activation.Torch.brelu</a></li>
<li><a class="reference internal" href="#module-Echo.Activation.Torch.functional" id="id102">Echo.Activation.Torch.functional</a></li>
<li><a class="reference internal" href="#indices-and-tables" id="id103">Indices and tables</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="about">
<h2><a class="toc-backref" href="#id77">About</a><a class="headerlink" href="#about" title="Permalink to this headline">¶</a></h2>
<p><strong>Echo Package</strong> is created to provide an implementation of the most promising mathematical algorithms, which are missing in the most popular deep learning libraries, such as <a class="reference external" href="https://pytorch.org/">PyTorch</a>, <a class="reference external" href="https://keras.io/">Keras</a> and
<a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a>.</p>
<div class="section" id="implemented-activation-functions">
<h3><a class="toc-backref" href="#id78">Implemented Activation Functions</a><a class="headerlink" href="#implemented-activation-functions" title="Permalink to this headline">¶</a></h3>
<p>List of activation functions implemented in Echo:</p>
<ol class="arabic simple">
<li>PyTorch:</li>
</ol>
<blockquote>
<div><ul class="simple">
<li>Weighted Tanh (see <a class="reference internal" href="#module-Echo.Activation.Torch.weightedTanh" title="Echo.Activation.Torch.weightedTanh"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.weightedTanh</span></code></a>)</li>
<li>Aria2 (see <a class="reference internal" href="#module-Echo.Activation.Torch.aria2" title="Echo.Activation.Torch.aria2"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.aria2</span></code></a>)</li>
<li>SiLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.silu" title="Echo.Activation.Torch.silu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.silu</span></code></a>)</li>
<li>E-Swish (see <a class="reference internal" href="#module-Echo.Activation.Torch.eswish" title="Echo.Activation.Torch.eswish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.eswish</span></code></a>)</li>
<li>Swish (see <a class="reference internal" href="#module-Echo.Activation.Torch.swish" title="Echo.Activation.Torch.swish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.swish</span></code></a>)</li>
<li>ELiSH (see <a class="reference internal" href="#module-Echo.Activation.Torch.elish" title="Echo.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.elish</span></code></a>)</li>
<li>Hard ELiSH (see <a class="reference internal" href="#module-Echo.Activation.Torch.hard_elish" title="Echo.Activation.Torch.hard_elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.hard_elish</span></code></a>)</li>
<li>Mila (see <a class="reference internal" href="#module-Echo.Activation.Torch.mila" title="Echo.Activation.Torch.mila"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mila</span></code></a>)</li>
<li>SineReLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.sine_relu" title="Echo.Activation.Torch.sine_relu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sine_relu</span></code></a>)</li>
<li>Flatten T-Swish (see <a class="reference internal" href="#module-Echo.Activation.Torch.fts" title="Echo.Activation.Torch.fts"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.fts</span></code></a>)</li>
<li>SQNL (see <a class="reference internal" href="#module-Echo.Activation.Torch.sqnl" title="Echo.Activation.Torch.sqnl"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sqnl</span></code></a>)</li>
<li>Mish (see <a class="reference internal" href="#module-Echo.Activation.Torch.mish" title="Echo.Activation.Torch.mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mish</span></code></a>)</li>
<li>ISRU (see <a class="reference internal" href="#module-Echo.Activation.Torch.isru" title="Echo.Activation.Torch.isru"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.isru</span></code></a>)</li>
<li>ISRLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.isrlu" title="Echo.Activation.Torch.isrlu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.isrlu</span></code></a>)</li>
<li>Bent’s Identity (see <a class="reference internal" href="#module-Echo.Activation.Torch.bent_id" title="Echo.Activation.Torch.bent_id"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.bent_id</span></code></a>)</li>
<li>Soft Clipping (see <a class="reference internal" href="#module-Echo.Activation.Torch.soft_clipping" title="Echo.Activation.Torch.soft_clipping"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.soft_clipping</span></code></a>)</li>
<li>SReLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.srelu" title="Echo.Activation.Torch.srelu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.srelu</span></code></a>)</li>
<li>BReLU (see <a class="reference internal" href="#module-Echo.Activation.Torch.brelu" title="Echo.Activation.Torch.brelu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.brelu</span></code></a>)</li>
<li>Beta Mish (see <a class="reference internal" href="#module-Echo.Activation.Torch.beta_mish" title="Echo.Activation.Torch.beta_mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.beta_mish</span></code></a>)</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="installation">
<h2><a class="toc-backref" href="#id79">Installation</a><a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>To install Echo package follow the instructions below:</p>
<ol class="arabic simple">
<li>Clone or download <a class="reference external" href="https://github.com/digantamisra98/Echo">GitHub repository</a>.</li>
<li>Navigate to <strong>Echo</strong> folder:</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; $ cd Echo
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li>Install the package with pip:</li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; $ pip install .
</pre></div>
</div>
</div>
<div class="section" id="torch-examples">
<h2><a class="toc-backref" href="#id80">Torch Examples</a><a class="headerlink" href="#torch-examples" title="Permalink to this headline">¶</a></h2>
<div class="section" id="activation-functions">
<h3><a class="toc-backref" href="#id81">Activation Functions</a><a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h3>
<p>The following code block contains an example of usage of an activation function
from Echo package:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># import activations from Echo</span>
<span class="hll"><span class="kn">from</span> <span class="nn">Echo.Activation.Torch.weightedTanh</span> <span class="kn">import</span> <span class="n">weightedTanh</span>
</span><span class="hll"><span class="kn">import</span> <span class="nn">Echo.Activation.Torch.functional</span> <span class="kn">as</span> <span class="nn">Func</span>
</span>
<span class="c1"># use activations in layers of model defined in class</span>
<span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># initialize layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># make sure the input tensor is flattened</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># apply activation function from Echo</span>
<span class="hll">        <span class="n">x</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">weighted_tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Initialize the model using defined Classifier class</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>

    <span class="c1"># Create model with Sequential</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
                         <span class="p">(</span><span class="s1">&#39;fc1&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span>
                         <span class="c1"># use activation function from Echo</span>
<span class="hll">                         <span class="p">(</span><span class="s1">&#39;wtahn1&#39;</span><span class="p">,</span>  <span class="n">weightedTanh</span><span class="p">(</span><span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)),</span>
</span>                         <span class="p">(</span><span class="s1">&#39;fc2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;bn2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">128</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;relu2&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                         <span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;fc3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;bn3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">64</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;relu3&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                         <span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span>
                         <span class="p">(</span><span class="s1">&#39;logsoftmax&#39;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="echo-api-reference">
<h2><a class="toc-backref" href="#id82">Echo API Reference</a><a class="headerlink" href="#echo-api-reference" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-Echo.Activation.Torch.aria2">
<span id="echo-activation-torch-aria2"></span><h3><a class="toc-backref" href="#id83">Echo.Activation.Torch.aria2</a><a class="headerlink" href="#module-Echo.Activation.Torch.aria2" title="Permalink to this headline">¶</a></h3>
<p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>See Aria paper:
<a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.aria2.aria2">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.aria2.</code><code class="descname">aria2</code><span class="sig-paren">(</span><em>beta=1</em>, <em>alpha=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/aria2.html#aria2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.aria2.aria2" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/aria2.png" src="_images/aria2.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id1"><span class="problematic" id="id2">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id3"><span class="problematic" id="id4">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyper-parameter which has a two-fold effect; it reduces the curvature in 3rd quadrant as well as increases the curvature in first quadrant while lowering the value of activation (default = 1)</li>
<li>beta: the exponential growth rate (default = 0.5)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See Aria paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/abs/1805.08878">https://arxiv.org/abs/1805.08878</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">aria2</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.aria2.aria2.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/aria2.html#aria2.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.aria2.aria2.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.mish">
<span id="echo-activation-torch-mish"></span><h3><a class="toc-backref" href="#id84">Echo.Activation.Torch.mish</a><a class="headerlink" href="#module-Echo.Activation.Torch.mish" title="Permalink to this headline">¶</a></h3>
<p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.mish.mish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.mish.</code><code class="descname">mish</code><a class="reference internal" href="_modules/Echo/Activation/Torch/mish.html#mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.mish.mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mish.png" src="_images/mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id5"><span class="problematic" id="id6">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id7"><span class="problematic" id="id8">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">mish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.mish.mish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/mish.html#mish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.mish.mish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.beta_mish">
<span id="echo-activation-torch-beta-mish"></span><h3><a class="toc-backref" href="#id85">Echo.Activation.Torch.beta_mish</a><a class="headerlink" href="#module-Echo.Activation.Torch.beta_mish" title="Permalink to this headline">¶</a></h3>
<p>Applies the β mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.beta_mish.beta_mish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.beta_mish.</code><code class="descname">beta_mish</code><span class="sig-paren">(</span><em>beta=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/beta_mish.html#beta_mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.beta_mish.beta_mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the β mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/beta_mish.png" src="_images/beta_mish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id9"><span class="problematic" id="id10">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id11"><span class="problematic" id="id12">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: hyperparameter (default = 1.5)</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">beta_mish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.beta_mish.beta_mish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/beta_mish.html#beta_mish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.beta_mish.beta_mish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.silu">
<span id="echo-activation-torch-silu"></span><h3><a class="toc-backref" href="#id86">Echo.Activation.Torch.silu</a><a class="headerlink" href="#module-Echo.Activation.Torch.silu" title="Permalink to this headline">¶</a></h3>
<p>Applies the Sigmoid Linear Unit (SiLU) function element-wise:</p>
<div class="math notranslate nohighlight">
\[silu(x) = x * sigmoid(x)\]</div>
<p>See related paper:
<a class="reference external" href="https://arxiv.org/pdf/1606.08415.pdf">https://arxiv.org/pdf/1606.08415.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.silu.silu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.silu.</code><code class="descname">silu</code><a class="reference internal" href="_modules/Echo/Activation/Torch/silu.html#silu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.silu.silu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Sigmoid Linear Unit (SiLU) function element-wise:</p>
<div class="math notranslate nohighlight">
\[silu(x) = x * sigmoid(x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/silu.png" src="_images/silu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id13"><span class="problematic" id="id14">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id15"><span class="problematic" id="id16">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1606.08415.pdf">https://arxiv.org/pdf/1606.08415.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">silu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.silu.silu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/silu.html#silu.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.silu.silu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.eswish">
<span id="echo-activation-torch-eswish"></span><h3><a class="toc-backref" href="#id87">Echo.Activation.Torch.eswish</a><a class="headerlink" href="#module-Echo.Activation.Torch.eswish" title="Permalink to this headline">¶</a></h3>
<p>Applies the E-Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>See E-Swish paper:
<a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.eswish.eswish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.eswish.</code><code class="descname">eswish</code><span class="sig-paren">(</span><em>beta=1.75</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/eswish.html#eswish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.eswish.eswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the E-Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/eswish.png" src="_images/eswish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id17"><span class="problematic" id="id18">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id19"><span class="problematic" id="id20">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/abs/1801.07145">https://arxiv.org/abs/1801.07145</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">eswish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.375</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.eswish.eswish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/eswish.html#eswish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.eswish.eswish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.swish">
<span id="echo-activation-torch-swish"></span><h3><a class="toc-backref" href="#id88">Echo.Activation.Torch.swish</a><a class="headerlink" href="#module-Echo.Activation.Torch.swish" title="Permalink to this headline">¶</a></h3>
<p>Applies the Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>See Swish paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.swish.swish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.swish.</code><code class="descname">swish</code><span class="sig-paren">(</span><em>beta=1.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/swish.html#swish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.swish.swish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Swish function element-wise:</p>
<div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/swish.png" src="_images/swish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id21"><span class="problematic" id="id22">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id23"><span class="problematic" id="id24">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>beta: hyperparameter, which controls the shape of the bump (default = 1.25)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">swish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.25</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.swish.swish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/swish.html#swish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.swish.swish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.elish">
<span id="echo-activation-torch-elish"></span><h3><a class="toc-backref" href="#id89">Echo.Activation.Torch.elish</a><a class="headerlink" href="#module-Echo.Activation.Torch.elish" title="Permalink to this headline">¶</a></h3>
<p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See ELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1808.00783.pdf">https://arxiv.org/pdf/1808.00783.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.elish.elish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.elish.</code><code class="descname">elish</code><a class="reference internal" href="_modules/Echo/Activation/Torch/elish.html#elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.elish.elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/elish.png" src="_images/elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id25"><span class="problematic" id="id26">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id27"><span class="problematic" id="id28">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>Related paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">elish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.elish.elish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/elish.html#elish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.elish.elish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.hard_elish">
<span id="echo-activation-torch-hard-elish"></span><h3><a class="toc-backref" href="#id90">Echo.Activation.Torch.hard_elish</a><a class="headerlink" href="#module-Echo.Activation.Torch.hard_elish" title="Permalink to this headline">¶</a></h3>
<p>Applies the HardELiSH function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See HardELiSH paper:
<a class="reference external" href="https://arxiv.org/pdf/1808.00783.pdf">https://arxiv.org/pdf/1808.00783.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.hard_elish.hard_elish">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.hard_elish.</code><code class="descname">hard_elish</code><a class="reference internal" href="_modules/Echo/Activation/Torch/hard_elish.html#hard_elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.hard_elish.hard_elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardELiSH function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_static/hard_elish.png" src="_static/hard_elish.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id29"><span class="problematic" id="id30">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id31"><span class="problematic" id="id32">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See HardELiSH paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.05941.pdf">https://arxiv.org/pdf/1710.05941.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">hard_elish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.hard_elish.hard_elish.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/hard_elish.html#hard_elish.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.hard_elish.hard_elish.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.mila">
<span id="echo-activation-torch-mila"></span><h3><a class="toc-backref" href="#id91">Echo.Activation.Torch.mila</a><a class="headerlink" href="#module-Echo.Activation.Torch.mila" title="Permalink to this headline">¶</a></h3>
<p>Applies the Mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x))\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.mila.mila">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.mila.</code><code class="descname">mila</code><span class="sig-paren">(</span><em>beta=-0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/mila.html#mila"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.mila.mila" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(ln(1 + e^{\beta + x})) = x * tanh(softplus(\beta + x)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/mila.png" src="_images/mila.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id33"><span class="problematic" id="id34">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id35"><span class="problematic" id="id36">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">mila</span><span class="p">(</span><span class="n">beta</span><span class="o">=-</span><span class="mf">0.25</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.mila.mila.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/mila.html#mila.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.mila.mila.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.sine_relu">
<span id="echo-activation-torch-sine-relu"></span><h3><a class="toc-backref" href="#id92">Echo.Activation.Torch.sine_relu</a><a class="headerlink" href="#module-Echo.Activation.Torch.sine_relu" title="Permalink to this headline">¶</a></h3>
<p>Applies the SineReLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>See related Medium article:
<a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.sine_relu.sine_relu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.sine_relu.</code><code class="descname">sine_relu</code><a class="reference internal" href="_modules/Echo/Activation/Torch/sine_relu.html#sine_relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.sine_relu.sine_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SineReLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sine_relu.png" src="_images/sine_relu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id37"><span class="problematic" id="id38">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id39"><span class="problematic" id="id40">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See related Medium article:</li>
</ul>
<p class="last"><a class="reference external" href="https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d">https://medium.com/&#64;wilder.rodrigues/sinerelu-an-alternative-to-the-relu-activation-function-e46a6199997d</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">sine_relu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.sine_relu.sine_relu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/sine_relu.html#sine_relu.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.sine_relu.sine_relu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.fts">
<span id="echo-activation-torch-fts"></span><h3><a class="toc-backref" href="#id93">Echo.Activation.Torch.fts</a><a class="headerlink" href="#module-Echo.Activation.Torch.fts" title="Permalink to this headline">¶</a></h3>
<p>Applies the FTS (Flatten T-Swish) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See Flatten T-Swish paper:
<a class="reference external" href="https://arxiv.org/pdf/1812.06247.pdf">https://arxiv.org/pdf/1812.06247.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.fts.fts">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.fts.</code><code class="descname">fts</code><a class="reference internal" href="_modules/Echo/Activation/Torch/fts.html#fts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.fts.fts" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the FTS (Flatten T-Swish) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/fts.png" src="_images/fts.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id41"><span class="problematic" id="id42">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id43"><span class="problematic" id="id44">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See Flattened T-Swish paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1812.06247.pdf">https://arxiv.org/pdf/1812.06247.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">fts</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.fts.fts.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/fts.html#fts.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.fts.fts.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.sqnl">
<span id="echo-activation-torch-sqnl"></span><h3><a class="toc-backref" href="#id94">Echo.Activation.Torch.sqnl</a><a class="headerlink" href="#module-Echo.Activation.Torch.sqnl" title="Permalink to this headline">¶</a></h3>
<p>Applies the SQNL function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>See SQNL paper:
<a class="reference external" href="https://ieeexplore.ieee.org/document/8489043">https://ieeexplore.ieee.org/document/8489043</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.sqnl.sqnl">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.sqnl.</code><code class="descname">sqnl</code><a class="reference internal" href="_modules/Echo/Activation/Torch/sqnl.html#sqnl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.sqnl.sqnl" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SQNL function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sqnl.png" src="_images/sqnl.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id45"><span class="problematic" id="id46">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id47"><span class="problematic" id="id48">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See SQNL paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://ieeexplore.ieee.org/document/8489043">https://ieeexplore.ieee.org/document/8489043</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">sqnl</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.sqnl.sqnl.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/sqnl.html#sqnl.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.sqnl.sqnl.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.isru">
<span id="echo-activation-torch-isru"></span><h3><a class="toc-backref" href="#id95">Echo.Activation.Torch.isru</a><a class="headerlink" href="#module-Echo.Activation.Torch.isru" title="Permalink to this headline">¶</a></h3>
<p>Applies the ISRU (Inverse Square Root Unit) function element-wise:</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>ISRU paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.isru.isru">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.isru.</code><code class="descname">isru</code><span class="sig-paren">(</span><em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/isru.html#isru"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.isru.isru" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRU function element-wise:</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isru.png" src="_images/isru.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id49"><span class="problematic" id="id50">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id51"><span class="problematic" id="id52">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>ISRU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">isru</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.isru.isru.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/isru.html#isru.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.isru.isru.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.isrlu">
<span id="echo-activation-torch-isrlu"></span><h3><a class="toc-backref" href="#id96">Echo.Activation.Torch.isrlu</a><a class="headerlink" href="#module-Echo.Activation.Torch.isrlu" title="Permalink to this headline">¶</a></h3>
<p>Applies the ISRLU (Inverse Square Root Linear Unit) function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>ISRLU paper:
<a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.isrlu.isrlu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.isrlu.</code><code class="descname">isrlu</code><span class="sig-paren">(</span><em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/isrlu.html#isrlu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.isrlu.isrlu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/isrlu.png" src="_images/isrlu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id53"><span class="problematic" id="id54">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id55"><span class="problematic" id="id56">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyperparameter α controls the value to which an ISRLU saturates for negative inputs (default = 1)</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li>ISRLU paper: <a class="reference external" href="https://arxiv.org/pdf/1710.09967.pdf">https://arxiv.org/pdf/1710.09967.pdf</a></li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">isrlu</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.isrlu.isrlu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/isrlu.html#isrlu.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.isrlu.isrlu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.bent_id">
<span id="echo-activation-torch-bent-id"></span><h3><a class="toc-backref" href="#id97">Echo.Activation.Torch.bent_id</a><a class="headerlink" href="#module-Echo.Activation.Torch.bent_id" title="Permalink to this headline">¶</a></h3>
<p>Applies the Bent’s Identity function element-wise:</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.bent_id.bent_id">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.bent_id.</code><code class="descname">bent_id</code><a class="reference internal" href="_modules/Echo/Activation/Torch/bent_id.html#bent_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.bent_id.bent_id" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Bent’s Identity function element-wise:</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/bent_id.png" src="_images/bent_id.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id57"><span class="problematic" id="id58">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id59"><span class="problematic" id="id60">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">bent_id</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.bent_id.bent_id.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/bent_id.html#bent_id.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.bent_id.bent_id.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="echo-activation-torch-soft-clipping">
<h3><a class="toc-backref" href="#id98">Echo.Activation.Torch.soft_clipping</a><a class="headerlink" href="#echo-activation-torch-soft-clipping" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-Echo.Activation.Torch.soft_clipping"></span><p>Applies Soft Clipping function element-wise:</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>See SC paper:
<a class="reference external" href="https://arxiv.org/pdf/1810.11509.pdf">https://arxiv.org/pdf/1810.11509.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.soft_clipping.soft_clipping">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.soft_clipping.</code><code class="descname">soft_clipping</code><span class="sig-paren">(</span><em>alpha=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/soft_clipping.html#soft_clipping"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.soft_clipping.soft_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Soft Clipping function element-wise:</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/sc.png" src="_images/sc.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id61"><span class="problematic" id="id62">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id63"><span class="problematic" id="id64">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>alpha: hyper-parameter, which determines how close to linear the central region is and how sharply the linear region turns to the asymptotic values</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>See SC paper:</dt>
<dd><a class="reference external" href="https://arxiv.org/pdf/1810.11509.pdf">https://arxiv.org/pdf/1810.11509.pdf</a></dd>
</dl>
</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">soft_clipping</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.soft_clipping.soft_clipping.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/soft_clipping.html#soft_clipping.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.soft_clipping.soft_clipping.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="echo-activation-torch-weightedtanh">
<h3><a class="toc-backref" href="#id99">Echo.Activation.Torch.weightedTanh</a><a class="headerlink" href="#echo-activation-torch-weightedtanh" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-Echo.Activation.Torch.weightedTanh"></span><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<dl class="class">
<dt id="Echo.Activation.Torch.weightedTanh.weightedTanh">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.weightedTanh.</code><code class="descname">weightedTanh</code><span class="sig-paren">(</span><em>weight=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/weightedTanh.html#weightedTanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.weightedTanh.weightedTanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/weighted_tanh.png" src="_images/weighted_tanh.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id65"><span class="problematic" id="id66">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id67"><span class="problematic" id="id68">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>Arguments:</dt>
<dd><ul class="first last simple">
<li>weight: hyperparameter (default = 1.0)</li>
</ul>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">weightedTanh</span><span class="p">(</span><span class="n">weight</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.weightedTanh.weightedTanh.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/weightedTanh.html#weightedTanh.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.weightedTanh.weightedTanh.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.srelu">
<span id="echo-activation-torch-srelu"></span><h3><a class="toc-backref" href="#id100">Echo.Activation.Torch.srelu</a><a class="headerlink" href="#module-Echo.Activation.Torch.srelu" title="Permalink to this headline">¶</a></h3>
<p>Script defined the SReLU (S-shaped Rectified Linear Activation Unit):</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x_i) = \left\{\begin{matrix} t_i^r + a_i^r(x_i - t_i^r), x_i \geq t_i^r \\  x_i, t_i^r &gt; x_i &gt; t_i^l\\  t_i^l + a_i^l(x_i - t_i^l), x_i \leq  t_i^l \\ \end{matrix}\right.\end{split}\]</div>
<p>See SReLU paper:
<a class="reference external" href="https://arxiv.org/pdf/1512.07030.pdf">https://arxiv.org/pdf/1512.07030.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.srelu.srelu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.srelu.</code><code class="descname">srelu</code><span class="sig-paren">(</span><em>in_features</em>, <em>parameters=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/srelu.html#srelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.srelu.srelu" title="Permalink to this definition">¶</a></dt>
<dd><p>SReLU (S-shaped Rectified Linear Activation Unit): a combination of three linear functions, which perform mapping R → R with the following formulation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h(x_i) = \left\{\begin{matrix} t_i^r + a_i^r(x_i - t_i^r), x_i \geq t_i^r \\  x_i, t_i^r &gt; x_i &gt; t_i^l\\  t_i^l + a_i^l(x_i - t_i^l), x_i \leq  t_i^l \\ \end{matrix}\right.\end{split}\]</div>
<p>with 4 trainable parameters.</p>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id69"><span class="problematic" id="id70">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id71"><span class="problematic" id="id72">*</span></a>), same shape as the input</li>
</ul>
</dd>
</dl>
<p>Parameters:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\{t_i^r, a_i^r, t_i^l, a_i^l\}\]</div>
</div></blockquote>
<p>4 trainable parameters, which model an individual SReLU activation unit. The subscript i indicates that we allow SReLU to vary in different channels. Parameters can be initialized manually or randomly.</p>
<dl class="docutils">
<dt>References:</dt>
<dd><ul class="first simple">
<li>See SReLU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1512.07030.pdf">https://arxiv.org/pdf/1512.07030.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">srelu_activation</span> <span class="o">=</span> <span class="n">srelu</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">srelu_activation</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="method">
<dt id="Echo.Activation.Torch.srelu.srelu.forward">
<code class="descname">forward</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/srelu.html#srelu.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.srelu.srelu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass of the function</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.brelu">
<span id="echo-activation-torch-brelu"></span><h3><a class="toc-backref" href="#id101">Echo.Activation.Torch.brelu</a><a class="headerlink" href="#module-Echo.Activation.Torch.brelu" title="Permalink to this headline">¶</a></h3>
<p>Script defined the BReLU (Bipolar Rectified Linear Activation Unit):</p>
<div class="math notranslate nohighlight">
\[\begin{split}BReLU(x_i) = \left\{\begin{matrix} f(x_i), i \mod 2 = 0\\  - f(-x_i), i \mod 2 \neq  0 \end{matrix}\right.\end{split}\]</div>
<p>See BReLU paper:
<a class="reference external" href="https://arxiv.org/pdf/1709.04054.pdf">https://arxiv.org/pdf/1709.04054.pdf</a></p>
<dl class="class">
<dt id="Echo.Activation.Torch.brelu.brelu">
<em class="property">class </em><code class="descclassname">Echo.Activation.Torch.brelu.</code><code class="descname">brelu</code><a class="reference internal" href="_modules/Echo/Activation/Torch/brelu.html#brelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.brelu.brelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation of BReLU activation function:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}BReLU(x_i) = \left\{\begin{matrix} f(x_i), i \mod 2 = 0\\  - f(-x_i), i \mod 2 \neq  0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>Plot:</p>
<div class="figure align-center">
<img alt="_images/brelu.png" src="_images/brelu.png" />
</div>
<dl class="docutils">
<dt>Shape:</dt>
<dd><ul class="first last simple">
<li>Input: (N, <a href="#id73"><span class="problematic" id="id74">*</span></a>) where * means, any number of additional
dimensions</li>
<li>Output: (N, <a href="#id75"><span class="problematic" id="id76">*</span></a>), same shape as the input</li>
</ul>
</dd>
<dt>References:</dt>
<dd><ul class="first simple">
<li>See BReLU paper:</li>
</ul>
<p class="last"><a class="reference external" href="https://arxiv.org/pdf/1709.04054.pdf">https://arxiv.org/pdf/1709.04054.pdf</a></p>
</dd>
<dt>Examples:</dt>
<dd><div class="first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">brelu_activation</span> <span class="o">=</span> <span class="n">brelu</span><span class="o">.</span><span class="n">apply</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t</span> <span class="o">=</span> <span class="n">brelu_activation</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="staticmethod">
<dt id="Echo.Activation.Torch.brelu.brelu.backward">
<em class="property">static </em><code class="descname">backward</code><span class="sig-paren">(</span><em>ctx</em>, <em>grad_output</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/brelu.html#brelu.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.brelu.brelu.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the backward pass we receive a Tensor containing the gradient of the loss
with respect to the output, and we need to compute the gradient of the loss
with respect to the input.</p>
</dd></dl>

<dl class="staticmethod">
<dt id="Echo.Activation.Torch.brelu.brelu.forward">
<em class="property">static </em><code class="descname">forward</code><span class="sig-paren">(</span><em>ctx</em>, <em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/brelu.html#brelu.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.brelu.brelu.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>In the forward pass we receive a Tensor containing the input and return
a Tensor containing the output. ctx is a context object that can be used
to stash information for backward computation. You can cache arbitrary
objects for use in the backward pass using the ctx.save_for_backward method.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-Echo.Activation.Torch.functional">
<span id="echo-activation-torch-functional"></span><h3><a class="toc-backref" href="#id102">Echo.Activation.Torch.functional</a><a class="headerlink" href="#module-Echo.Activation.Torch.functional" title="Permalink to this headline">¶</a></h3>
<p>Script provides functional interface for custom activation functions.</p>
<dl class="function">
<dt id="Echo.Activation.Torch.functional.aria2">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">aria2</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>alpha=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#aria2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.aria2" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Aria-2 function element-wise:</p>
<div class="math notranslate nohighlight">
\[Aria2(x, \alpha, \beta) = (1+e^{-\beta*x})^{-\alpha}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.aria2" title="Echo.Activation.Torch.aria2"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.aria2</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.bent_id">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">bent_id</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#bent_id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.bent_id" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Bent’s Identity function element-wise:</p>
<div class="math notranslate nohighlight">
\[bentId(x) = x + \frac{\sqrt{x^{2}+1}-1}{2}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.bent_id" title="Echo.Activation.Torch.bent_id"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.bent_id</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.beta_mish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">beta_mish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#beta_mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.beta_mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the β mish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\beta mish(x) = x * tanh(ln((1 + e^{x})^{\beta}))\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.beta_mish" title="Echo.Activation.Torch.beta_mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.beta_mish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.elish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">elish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.elish" title="Echo.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.elish</span></code></a>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}ELiSH(x) = \left\{\begin{matrix} x / (1+e^{-x}), x \geq 0 \\ (e^{x} - 1) / (1 + e^{-x}), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.elish" title="Echo.Activation.Torch.elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.elish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.eswish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">eswish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.75</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#eswish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.eswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the E-Swish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ESwish(x, \beta) = \beta*x*sigmoid(x)\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.eswish" title="Echo.Activation.Torch.eswish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.eswish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.fts">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">fts</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#fts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.fts" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the FTS (Flatten T-Swish) activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}FTS(x) = \left\{\begin{matrix} \frac{x}{1 + e^{-x}} , x \geq  0 \\ 0, x &lt; 0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.fts" title="Echo.Activation.Torch.fts"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.fts</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.hard_elish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">hard_elish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#hard_elish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.hard_elish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardELiSH (Exponential Linear Sigmoid SquasHing) function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}HardELiSH(x) = \left\{\begin{matrix} x \times max(0, min(1, (x + 1) / 2)), x \geq 0 \\ (e^{x} - 1)\times max(0, min(1, (x + 1) / 2)), x &lt; 0 \end{matrix}\right.\end{split}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.hard_elish" title="Echo.Activation.Torch.hard_elish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.hard_elish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.isrlu">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">isrlu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#isrlu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.isrlu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRLU function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}ISRLU(x)=\left\{\begin{matrix} x, x\geq 0 \\  x * (\frac{1}{\sqrt{1 + \alpha*x^2}}), x &lt;0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.isrlu" title="Echo.Activation.Torch.isrlu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.isrlu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.isru">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">isru</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#isru"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.isru" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the ISRU function element-wise:</p>
<div class="math notranslate nohighlight">
\[ISRU(x) = \frac{x}{\sqrt{1 + \alpha * x^2}}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.isru" title="Echo.Activation.Torch.isru"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.isru</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.mila">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">mila</code><span class="sig-paren">(</span><em>input</em>, <em>beta=-0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#mila"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.mila" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mila function element-wise:</p>
<div class="math notranslate nohighlight">
\[mila(x) = x * tanh(softplus(\beta + x)) = x * tanh(ln(1 + e^{\beta + x}))\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.mila" title="Echo.Activation.Torch.mila"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mila</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.mish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">mish</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#mish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.mish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the mish function element-wise:</p>
<div class="math notranslate nohighlight">
\[mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.mish" title="Echo.Activation.Torch.mish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.mish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.silu">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">silu</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#silu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.silu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Sigmoid Linear Unit (SiLU) function element-wise:</p>
<div class="math notranslate nohighlight">
\[SiLU(x) = x * sigmoid(x)\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.silu" title="Echo.Activation.Torch.silu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.silu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.sineReLU">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">sineReLU</code><span class="sig-paren">(</span><em>input</em>, <em>eps=0.01</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#sineReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.sineReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SineReLU activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SineReLU(x, \epsilon) = \left\{\begin{matrix} x , x &gt; 0 \\ \epsilon * (sin(x) - cos(x)), x \leq  0 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.sine_relu" title="Echo.Activation.Torch.sine_relu"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sine_relu</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.soft_clipping">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">soft_clipping</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#soft_clipping"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.soft_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Soft Clipping function element-wise:</p>
<div class="math notranslate nohighlight">
\[SC(x) = 1 / \alpha * log(\frac{1 + e^{\alpha * x}}{1 + e^{\alpha * (x-1)}})\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.soft_clipping" title="Echo.Activation.Torch.soft_clipping"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.soft_clipping</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.sqnl">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">sqnl</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#sqnl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.sqnl" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the SQNL activation function element-wise:</p>
<div class="math notranslate nohighlight">
\[\begin{split}SQNL(x) = \left\{\begin{matrix} 1, x &gt; 2 \\ x - \frac{x^2}{4}, 0 \leq x \leq 2 \\  x + \frac{x^2}{4}, -2 \leq x &lt; 0 \\ -1, x &lt; -2 \end{matrix}\right.\end{split}\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.sqnl" title="Echo.Activation.Torch.sqnl"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.sqnl</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.swish">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">swish</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#swish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.swish" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the Swish function element-wise:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[Swish(x, \beta) = x*sigmoid(\beta*x) = \frac{x}{(1+e^{-\beta*x})}\]</div>
</div></blockquote>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.swish" title="Echo.Activation.Torch.swish"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.swish</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="Echo.Activation.Torch.functional.weighted_tanh">
<code class="descclassname">Echo.Activation.Torch.functional.</code><code class="descname">weighted_tanh</code><span class="sig-paren">(</span><em>input</em>, <em>weight=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/Echo/Activation/Torch/functional.html#weighted_tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#Echo.Activation.Torch.functional.weighted_tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the weighted tanh function element-wise:</p>
<div class="math notranslate nohighlight">
\[weightedtanh(x) = tanh(x * weight)\]</div>
<p>See additional documentation for <a class="reference internal" href="#module-Echo.Activation.Torch.weightedTanh" title="Echo.Activation.Torch.weightedTanh"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Echo.Activation.Torch.weightedTanh</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="indices-and-tables">
<h3><a class="toc-backref" href="#id103">Indices and tables</a><a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="#">
    <img class="logo" src="_static/echo_logo.png" alt="Logo"/>
    
  </a>
</p>






<p>
<iframe src="https://ghbtns.com/github-btn.html?user=digantamisra98&repo=Echo&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Diganta Misra, Aleksandra Deis.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>